{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsteiner93/rubiks-ml/blob/main/toy_gnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we define a toy graph problem and solve it using implementations of graph learning methods from scratch as well as the framework [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/)."
      ],
      "metadata": {
        "id": "Eytkd7ViMVVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVi0X-6il1vi"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import dataclasses\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrZ9zcTYmJks"
      },
      "source": [
        "### Define a toy graph problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J02jEnZWn509"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed: int) -> None:\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed + 1)\n",
        "  torch.manual_seed(seed + 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrW12h4vmICS"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass(frozen=True, kw_only=True)\n",
        "class Node:\n",
        "  node_id: int\n",
        "  node_feature_a: int  # Must be in [0, node_feature_a_size]\n",
        "  node_feature_b: float  # Must be in [-1.0, 1.0]\n",
        "  distractor_feature: int  # Must be in [-1, 1]\n",
        "\n",
        "  def __post_init__(self):\n",
        "    assert self.node_feature_a >= 0\n",
        "    assert self.node_feature_b >= -1.0 and self.node_feature_b <= 1.0\n",
        "    assert self.distractor_feature >= -1 and self.distractor_feature <= 1\n",
        "\n",
        "  def get_feature_vector(self, *, node_feature_a_size: int) -> np.ndarray:\n",
        "    assert self.node_feature_a <= node_feature_a_size\n",
        "\n",
        "    feature_vector = np.zeros(node_feature_a_size + 2, dtype=np.float32)\n",
        "    feature_vector[self.node_feature_a] = 1.0\n",
        "    feature_vector[-2] = self.node_feature_b\n",
        "    feature_vector[-1] = self.distractor_feature\n",
        "    return feature_vector\n",
        "\n",
        "  def get_feature_vector_jnp(self, *, node_feature_a_size: int) -> jnp.ndarray:\n",
        "    return jnp.array(self.get_feature_vector(node_feature_a_size=node_feature_a_size))\n",
        "\n",
        "@dataclasses.dataclass(frozen=True, kw_only=True)\n",
        "class UndirectedEdge:\n",
        "  # By convention, we'll always put the lower id as the src and higher id as dst.\n",
        "  src: int\n",
        "  dst: int\n",
        "\n",
        "class Graph:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.id_to_node: dict[int, Node] = {}\n",
        "    self.edges: set[UndirectedEdge] = set()\n",
        "    self.adjacency_list: dict[int, set[int]] = collections.defaultdict(set)\n",
        "\n",
        "  def add_node(self, node_to_add: Node) -> None:\n",
        "    if node_to_add.node_id in self.id_to_node:\n",
        "      raise ValueError(f\"Node with id {node_to_add.node_id} is already in the graph.\")\n",
        "    self.id_to_node[node_to_add.node_id] = node_to_add\n",
        "\n",
        "  def add_edge(self, src: int, dst: int) -> None:\n",
        "    if src not in self.id_to_node:\n",
        "      raise ValueError(f\"Source {src} not in node ids {self.id_to_node.keys()}\")\n",
        "    if dst not in self.id_to_node:\n",
        "      raise ValueError(f\"Destination {dst} not in node ids {self.id_to_node.keys()}\")\n",
        "    if dst < src:\n",
        "      tmp = src\n",
        "      src = dst\n",
        "      dst = tmp\n",
        "    edge_to_add = UndirectedEdge(src=src, dst=dst)\n",
        "    self.edges.add(edge_to_add)\n",
        "    self.adjacency_list[src].add(dst)\n",
        "    # Because it is undirected, the edge needs to go both ways.\n",
        "    self.adjacency_list[dst].add(src)\n",
        "\n",
        "  def get_adjacency_matrix(self, add_self_loops: bool = False) -> np.ndarray:\n",
        "    if sorted(list(self.id_to_node.keys())) != list(range(len(self.id_to_node))):\n",
        "      raise ValueError(f\"Node ids {sorted(list(self.id_to_node.keys()))} must be range(len(id_to_node)).\")\n",
        "    adjacency_matrix = np.zeros((len(self.id_to_node), len(self.id_to_node)), dtype=np.int32)\n",
        "    for node_id, neighbors in self.adjacency_list.items():\n",
        "      for neighbor in neighbors:\n",
        "        adjacency_matrix[node_id][neighbor] = 1\n",
        "      if add_self_loops:\n",
        "        adjacency_matrix[node_id][node_id] = 1\n",
        "    return adjacency_matrix\n",
        "\n",
        "  def get_adjacency_matrix_jnp(self, add_self_loops: bool = False) -> jnp.ndarray:\n",
        "    return jnp.array(self.get_adjacency_matrix(add_self_loops=add_self_loops))\n",
        "\n",
        "  def get_edge_connections_coo(self) -> torch.LongTensor:\n",
        "    # Used by PyTorch Geomtric.\n",
        "    # COO described here: https://docs.pytorch.org/docs/stable/sparse.html#sparse-coo-docs\n",
        "    # Since the graph is undirected, we need to specify the edges going in both directions.\n",
        "    a = torch.LongTensor([[e.src, e.dst] for e in self.edges]).t().reshape((2, -1))\n",
        "    b = torch.LongTensor([[e.dst, e.src] for e in self.edges]).t().reshape((2, -1))\n",
        "    return torch.cat([a, b], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gO26zY5qxzGL",
        "outputId": "6469a05c-dbf8-41cd-a9a0-693bb5235814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_get_feature_vector (__main__.NodeTest.test_get_feature_vector) ... ok\n",
            "test_invalid_node_rejected (__main__.NodeTest.test_invalid_node_rejected) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 1.696s\n",
            "\n",
            "OK\n",
            "test_graph_representations (__main__.GraphTest.test_graph_representations) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.078s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7d97d03fda10>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import unittest\n",
        "\n",
        "class NodeTest(unittest.TestCase):\n",
        "\n",
        "  def test_invalid_node_rejected(self):\n",
        "    with self.assertRaises(AssertionError):\n",
        "      node = Node(node_id=0, node_feature_a=1, node_feature_b=-2.0, distractor_feature=1)\n",
        "\n",
        "  def test_get_feature_vector(self):\n",
        "    node = Node(node_id=0, node_feature_a=5, node_feature_b=.2, distractor_feature=1)\n",
        "    expected = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, .2, 1], dtype=np.float32)\n",
        "    np.testing.assert_array_equal(expected, node.get_feature_vector(node_feature_a_size=10))\n",
        "    np.testing.assert_array_equal(expected, node.get_feature_vector_jnp(node_feature_a_size=10))\n",
        "\n",
        "class GraphTest(unittest.TestCase):\n",
        "\n",
        "  def test_graph_representations(self):\n",
        "    graph = Graph()\n",
        "    node0 = Node(node_id=0, node_feature_a=5, node_feature_b=.0, distractor_feature=1)\n",
        "    node1 = Node(node_id=1, node_feature_a=2, node_feature_b=-.5, distractor_feature=0)\n",
        "    node2 = Node(node_id=2, node_feature_a=6, node_feature_b=-.123, distractor_feature=1)\n",
        "    node3 = Node(node_id=3, node_feature_a=9, node_feature_b=.55, distractor_feature=-1)\n",
        "    graph.add_node(node0)\n",
        "    graph.add_node(node1)\n",
        "    graph.add_node(node2)\n",
        "    graph.add_node(node3)\n",
        "    graph.add_edge(0, 1)\n",
        "    graph.add_edge(0, 2)\n",
        "    graph.add_edge(2, 1)\n",
        "    graph.add_edge(3, 0)\n",
        "    expected_adjacency_matrix_no_self_loops = np.array([\n",
        "        [0, 1, 1, 1],\n",
        "        [1, 0, 1, 0],\n",
        "        [1, 1, 0, 0],\n",
        "        [1, 0, 0, 0],\n",
        "    ])\n",
        "    expected_adjacency_matrix_with_self_loops = np.array([\n",
        "        [1, 1, 1, 1],\n",
        "        [1, 1, 1, 0],\n",
        "        [1, 1, 1, 0],\n",
        "        [1, 0, 0, 1],\n",
        "    ])\n",
        "    np.testing.assert_array_equal(\n",
        "        expected_adjacency_matrix_no_self_loops, graph.get_adjacency_matrix(add_self_loops=False))\n",
        "    np.testing.assert_array_equal(\n",
        "        expected_adjacency_matrix_with_self_loops, graph.get_adjacency_matrix(add_self_loops=True))\n",
        "    np.testing.assert_array_equal(\n",
        "        expected_adjacency_matrix_no_self_loops, graph.get_adjacency_matrix_jnp(add_self_loops=False))\n",
        "    np.testing.assert_array_equal(\n",
        "        expected_adjacency_matrix_with_self_loops, graph.get_adjacency_matrix_jnp(add_self_loops=True))\n",
        "    coo = graph.get_edge_connections_coo()\n",
        "    self.assertEqual((2, 8), coo.shape)\n",
        "\n",
        "\n",
        "unittest.main(NodeTest(), argv=[''], verbosity=2, exit=False)\n",
        "unittest.main(GraphTest(), argv=[''], verbosity=2, exit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av8KoOLqmX00",
        "outputId": "d7ad6bfb-266c-44eb-f393-7274b52a6584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats for train set:\n",
            "Average graph size: 7.459\n",
            "Total node count: 74590\n",
            "Label 0. Count: 13973. Percentage: 0.19.\n",
            "Label 1. Count: 2120. Percentage: 0.03.\n",
            "Label 2. Count: 7907. Percentage: 0.11.\n",
            "Label 3. Count: 8231. Percentage: 0.11.\n",
            "Label 4. Count: 17112. Percentage: 0.23.\n",
            "Label 5. Count: 7622. Percentage: 0.10.\n",
            "Label 6. Count: 5056. Percentage: 0.07.\n",
            "Label 7. Count: 12569. Percentage: 0.17.\n",
            "\n",
            "Stats for test set:\n",
            "Average graph size: 7.538\n",
            "Total node count: 7538\n",
            "Label 0. Count: 1487. Percentage: 0.20.\n",
            "Label 1. Count: 166. Percentage: 0.02.\n",
            "Label 2. Count: 802. Percentage: 0.11.\n",
            "Label 3. Count: 749. Percentage: 0.10.\n",
            "Label 4. Count: 1736. Percentage: 0.23.\n",
            "Label 5. Count: 776. Percentage: 0.10.\n",
            "Label 6. Count: 531. Percentage: 0.07.\n",
            "Label 7. Count: 1291. Percentage: 0.17.\n"
          ]
        }
      ],
      "source": [
        "# We will construct random graphs of variable size for a toy node classification problem.\n",
        "# Each node will be assigned to one of 8 possible classes.\n",
        "\n",
        "# Each node has 3 features with the following rules:\n",
        "# node_feature_a is an int that can take values from [0, 9]\n",
        "# node_feature_b is a float that can take values from [-1, 1]\n",
        "# distractor_feature is an int that can take values in {-1, 0, 1}, but it is never relevant to making a correct prediction.\n",
        "\n",
        "# The output prediction is one of 8 classes {0, 1, 2, 3, 4, 5, 6, 7}.\n",
        "# If a node has exactly 3 neighbors (not including self), its class label should be 0.\n",
        "# Elif all of a node's neighbors within a 2-hop neighborhood (including self) have node_feature_b <= 0, it should be 1.\n",
        "# Elif >= 75% of a node's neighbors (including self) have an even value for node_feature_a, it should be 2.\n",
        "# Elif >= 75% of a node's neighbors (including self) have an odd value for node_feature_a, it should be 3.\n",
        "# Elif >= 50% of a node's neighbors (not including self) have node_feature_b > 0, it should be 4.\n",
        "# Elif node_feature_a is even and node_feature_b is <= 0, it should be 5.\n",
        "# Elif node_feature_a is odd and node_feature_b is > 0, it should be 6.\n",
        "# Everything else should be 7.\n",
        "\n",
        "set_seeds(1)\n",
        "\n",
        "node_feature_a_size = 10\n",
        "num_classes = 8\n",
        "\n",
        "def generate_random_graph(\n",
        "    mean_nodes: int = 8,\n",
        ") -> Graph:\n",
        "  num_nodes = max(1, int(np.random.normal(loc=mean_nodes, scale=2)))\n",
        "  graph = Graph()\n",
        "  for node_id in range(num_nodes):\n",
        "    node_feature_b = random.uniform(-1, 1)\n",
        "    if node_feature_b >= 0 and random.uniform(0, 1) <= .25:\n",
        "      # Make it a bit more likely for node_feature_b to be negative.\n",
        "      node_feature_b = -1.0\n",
        "    graph.add_node(\n",
        "        Node(node_id=node_id,\n",
        "             node_feature_a=random.randint(0, node_feature_a_size-1),\n",
        "             node_feature_b=node_feature_b,\n",
        "             distractor_feature=random.randint(-1, 1))\n",
        "    )\n",
        "  mean_edges = 4 * mean_nodes\n",
        "  for _ in range(mean_edges):\n",
        "    src = random.randint(0, num_nodes-1)\n",
        "    dst = random.randint(0, num_nodes-1)\n",
        "    if src == dst:\n",
        "      continue\n",
        "    graph.add_edge(src, dst)\n",
        "\n",
        "  return graph\n",
        "\n",
        "def get_node_labels_for_graph(graph: Graph) -> list[int]:\n",
        "  labels = []\n",
        "  num_nodes = len(graph.id_to_node)\n",
        "  for node_id in range(num_nodes):\n",
        "    label = -1\n",
        "\n",
        "    all_1_hop_neighbors_not_including_self = [e for e in graph.adjacency_list[node_id] if e != node_id]\n",
        "    num_neighbors_of_node_not_including_self = len(all_1_hop_neighbors_not_including_self)\n",
        "\n",
        "    all_2_hop_have_negative_node_feature_b = graph.id_to_node[node_id].node_feature_b <= 0\n",
        "    for one_hop_node_id in all_1_hop_neighbors_not_including_self:\n",
        "      one_hop_node = graph.id_to_node[one_hop_node_id]\n",
        "      if one_hop_node.node_feature_b > 0:\n",
        "        all_2_hop_have_negative_node_feature_b = False\n",
        "        break\n",
        "      for two_hop_node_id in graph.adjacency_list[one_hop_node_id]:\n",
        "        two_hop_node = graph.id_to_node[two_hop_node_id]\n",
        "        if two_hop_node.node_feature_b > 0:\n",
        "          all_2_hop_have_negative_node_feature_b = False\n",
        "          break\n",
        "\n",
        "    all_1_hop_neighbors_including_self = set(graph.adjacency_list[node_id])\n",
        "    all_1_hop_neighbors_including_self.add(node_id)\n",
        "    even_value_for_feature_a_neighbors = [e for e in all_1_hop_neighbors_including_self if graph.id_to_node[e].node_feature_a % 2 == 0]\n",
        "    odd_value_for_feature_a_neighbors = [e for e in all_1_hop_neighbors_including_self if graph.id_to_node[e].node_feature_a % 2 != 0]\n",
        "    positive_feature_b_neighbors_not_including_self = [e for e in all_1_hop_neighbors_not_including_self if graph.id_to_node[e].node_feature_b > 0]\n",
        "\n",
        "    if num_neighbors_of_node_not_including_self == 3:\n",
        "      label = 0\n",
        "    elif all_2_hop_have_negative_node_feature_b:\n",
        "      label = 1\n",
        "    elif float(len(even_value_for_feature_a_neighbors)) / len(all_1_hop_neighbors_including_self) >= .75:\n",
        "      label = 2\n",
        "    elif float(len(odd_value_for_feature_a_neighbors)) / len(all_1_hop_neighbors_including_self) >= .75:\n",
        "      label = 3\n",
        "    elif len(all_1_hop_neighbors_not_including_self) > 0 and (float(len(positive_feature_b_neighbors_not_including_self)) / len(all_1_hop_neighbors_not_including_self)) >= .5:\n",
        "      label = 4\n",
        "    elif graph.id_to_node[node_id].node_feature_a % 2 == 0 and graph.id_to_node[node_id].node_feature_b <= 0:\n",
        "      label = 5\n",
        "    elif graph.id_to_node[node_id].node_feature_a % 2 != 0 and graph.id_to_node[node_id].node_feature_b > 0:\n",
        "      label = 6\n",
        "    else:\n",
        "      label = 7\n",
        "\n",
        "    assert label >= 0 and label <= 7\n",
        "    labels.append(label)\n",
        "\n",
        "  assert len(labels) == num_nodes\n",
        "  return labels\n",
        "\n",
        "def get_train_and_test_set(*, train_set_size: int, test_set_size: int) -> tuple[list[tuple[Graph, list[int]]], list[tuple[Graph, list[int]]]]:\n",
        "  # Returns tuple of (train_set, test_set)\n",
        "  # train_set and test_set are both a list of pairs of Graphs and labels for all nodes in that graph (in order).\n",
        "  train_set = []\n",
        "  for _ in range(train_set_size):\n",
        "    graph = generate_random_graph()\n",
        "    labels = get_node_labels_for_graph(graph)\n",
        "    train_set.append((graph, labels))\n",
        "\n",
        "  test_set = []\n",
        "  for _ in range(test_set_size):\n",
        "    graph = generate_random_graph()\n",
        "    labels = get_node_labels_for_graph(graph)\n",
        "    test_set.append((graph, labels))\n",
        "\n",
        "  return train_set, test_set\n",
        "\n",
        "\n",
        "train_set, test_set = get_train_and_test_set(train_set_size=10000, test_set_size=1000)\n",
        "example_node = Node(node_id=0, node_feature_a=0, node_feature_b=0.0, distractor_feature=0)\n",
        "\n",
        "def print_statistics(set_to_check: list[tuple[Graph, list[int]]]) -> None:\n",
        "  all_graph_sizes = []\n",
        "  all_labels = collections.defaultdict(int)\n",
        "  for graph, labels in set_to_check:\n",
        "    all_graph_sizes.append(len(graph.id_to_node))\n",
        "    for label in labels:\n",
        "      all_labels[label] += 1\n",
        "\n",
        "  all_labels_count = sum(all_labels.values())\n",
        "  print(f\"Average graph size: {np.mean(all_graph_sizes)}\")\n",
        "  print(f\"Total node count: {all_labels_count}\")\n",
        "  for i in range(num_classes):\n",
        "    print(f\"Label {i}. Count: {all_labels[i]}. Percentage: {all_labels[i] / all_labels_count:.2f}.\")\n",
        "\n",
        "# Compute some statistics on the train and test sets.\n",
        "print(\"Stats for train set:\")\n",
        "print_statistics(train_set)\n",
        "print()\n",
        "print(\"Stats for test set:\")\n",
        "print_statistics(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ6t-Cs7xyUd"
      },
      "source": [
        "### Solve using only node features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76tJpRUjxdjS"
      },
      "outputs": [],
      "source": [
        "def convert_dataset_to_vectors(*, dataset: list[tuple[Graph, list[int]]], batch_size: int | None = None) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  all_node_vectors = []\n",
        "  all_labels = []\n",
        "  for graph, labels in dataset:\n",
        "    for node_id in range(len(graph.id_to_node)):\n",
        "      all_node_vectors.append(graph.id_to_node[node_id].get_feature_vector_jnp(node_feature_a_size=node_feature_a_size))\n",
        "    targets = jnp.array(labels).reshape(-1)\n",
        "    one_hot_targets = jnp.eye(num_classes)[targets]\n",
        "    all_labels.append(one_hot_targets)\n",
        "  stacked_node_features = jnp.vstack(all_node_vectors)\n",
        "  stacked_labels = jnp.vstack(all_labels)\n",
        "  if batch_size is None:\n",
        "    return stacked_node_features, stacked_labels\n",
        "\n",
        "  if stacked_node_features.shape[0] % batch_size != 0:\n",
        "    number_to_add = batch_size - (stacked_node_features.shape[0] % batch_size)\n",
        "    stacked_node_features = jnp.vstack([stacked_node_features, jnp.zeros((number_to_add, stacked_node_features.shape[1]))])\n",
        "  if stacked_labels.shape[0] % batch_size != 0:\n",
        "    number_to_add = batch_size - (stacked_labels.shape[0] % batch_size)\n",
        "    stacked_labels = jnp.vstack([stacked_labels, jnp.zeros((number_to_add, stacked_labels.shape[1]))])\n",
        "  batched_node_features = stacked_node_features.reshape((-1, batch_size, stacked_node_features.shape[1]))\n",
        "  batched_labels = stacked_labels.reshape((-1, batch_size, stacked_labels.shape[1]))\n",
        "  return batched_node_features, batched_labels\n",
        "\n",
        "\n",
        "batched_train_set, batched_train_labels = convert_dataset_to_vectors(dataset=train_set, batch_size=64)\n",
        "vectorized_test_set, vectorized_test_labels = convert_dataset_to_vectors(dataset=test_set, batch_size=None)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batched_train_set_torch = torch.Tensor(np.array(batched_train_set)).to(device)\n",
        "batched_train_labels_torch = torch.Tensor(np.array(batched_train_labels)).to(device)\n",
        "vectorized_test_set_torch = torch.Tensor(np.array(vectorized_test_set)).to(device)\n",
        "vectorized_test_labels_torch = torch.Tensor(np.array(vectorized_test_labels)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZyilqGi5ST0",
        "outputId": "ec2d5dd1-9810-4ca3-e186-2b483142f73a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning training for epoch 1 of 50...\n",
            "After 1 epochs, test accuracy was 0.32. Final batch loss was 43.4771.\n",
            "Beginning training for epoch 2 of 50...\n",
            "After 2 epochs, test accuracy was 0.33. Final batch loss was 43.1715.\n",
            "Beginning training for epoch 3 of 50...\n",
            "After 3 epochs, test accuracy was 0.33. Final batch loss was 43.1604.\n",
            "Beginning training for epoch 4 of 50...\n",
            "After 4 epochs, test accuracy was 0.33. Final batch loss was 43.2434.\n",
            "Beginning training for epoch 5 of 50...\n",
            "After 5 epochs, test accuracy was 0.33. Final batch loss was 43.2694.\n",
            "Beginning training for epoch 6 of 50...\n",
            "After 6 epochs, test accuracy was 0.34. Final batch loss was 43.2728.\n",
            "Beginning training for epoch 7 of 50...\n",
            "After 7 epochs, test accuracy was 0.34. Final batch loss was 43.3746.\n",
            "Beginning training for epoch 8 of 50...\n",
            "After 8 epochs, test accuracy was 0.34. Final batch loss was 43.4371.\n",
            "Beginning training for epoch 9 of 50...\n",
            "After 9 epochs, test accuracy was 0.34. Final batch loss was 43.5352.\n",
            "Beginning training for epoch 10 of 50...\n",
            "After 10 epochs, test accuracy was 0.34. Final batch loss was 43.5673.\n",
            "Beginning training for epoch 11 of 50...\n",
            "After 11 epochs, test accuracy was 0.34. Final batch loss was 43.6642.\n",
            "Beginning training for epoch 12 of 50...\n",
            "After 12 epochs, test accuracy was 0.34. Final batch loss was 43.6723.\n",
            "Beginning training for epoch 13 of 50...\n",
            "After 13 epochs, test accuracy was 0.34. Final batch loss was 43.6605.\n",
            "Beginning training for epoch 14 of 50...\n",
            "After 14 epochs, test accuracy was 0.34. Final batch loss was 43.6796.\n",
            "Beginning training for epoch 15 of 50...\n",
            "After 15 epochs, test accuracy was 0.34. Final batch loss was 43.7132.\n",
            "Beginning training for epoch 16 of 50...\n",
            "After 16 epochs, test accuracy was 0.34. Final batch loss was 43.7288.\n",
            "Beginning training for epoch 17 of 50...\n",
            "After 17 epochs, test accuracy was 0.34. Final batch loss was 43.7364.\n",
            "Beginning training for epoch 18 of 50...\n",
            "After 18 epochs, test accuracy was 0.34. Final batch loss was 43.7520.\n",
            "Beginning training for epoch 19 of 50...\n",
            "After 19 epochs, test accuracy was 0.34. Final batch loss was 43.7967.\n",
            "Beginning training for epoch 20 of 50...\n",
            "After 20 epochs, test accuracy was 0.34. Final batch loss was 43.8173.\n",
            "Beginning training for epoch 21 of 50...\n",
            "After 21 epochs, test accuracy was 0.34. Final batch loss was 43.9062.\n",
            "Beginning training for epoch 22 of 50...\n",
            "After 22 epochs, test accuracy was 0.34. Final batch loss was 43.9176.\n",
            "Beginning training for epoch 23 of 50...\n",
            "After 23 epochs, test accuracy was 0.34. Final batch loss was 43.8495.\n",
            "Beginning training for epoch 24 of 50...\n",
            "After 24 epochs, test accuracy was 0.34. Final batch loss was 43.8313.\n",
            "Beginning training for epoch 25 of 50...\n",
            "After 25 epochs, test accuracy was 0.34. Final batch loss was 43.8317.\n",
            "Beginning training for epoch 26 of 50...\n",
            "After 26 epochs, test accuracy was 0.34. Final batch loss was 43.8326.\n",
            "Beginning training for epoch 27 of 50...\n",
            "After 27 epochs, test accuracy was 0.34. Final batch loss was 43.8223.\n",
            "Beginning training for epoch 28 of 50...\n",
            "After 28 epochs, test accuracy was 0.34. Final batch loss was 43.8329.\n",
            "Beginning training for epoch 29 of 50...\n",
            "After 29 epochs, test accuracy was 0.34. Final batch loss was 43.8335.\n",
            "Beginning training for epoch 30 of 50...\n",
            "After 30 epochs, test accuracy was 0.34. Final batch loss was 43.8360.\n",
            "Beginning training for epoch 31 of 50...\n",
            "After 31 epochs, test accuracy was 0.34. Final batch loss was 43.8576.\n",
            "Beginning training for epoch 32 of 50...\n",
            "After 32 epochs, test accuracy was 0.34. Final batch loss was 43.8647.\n",
            "Beginning training for epoch 33 of 50...\n",
            "After 33 epochs, test accuracy was 0.34. Final batch loss was 43.8667.\n",
            "Beginning training for epoch 34 of 50...\n",
            "After 34 epochs, test accuracy was 0.34. Final batch loss was 43.8401.\n",
            "Beginning training for epoch 35 of 50...\n",
            "After 35 epochs, test accuracy was 0.34. Final batch loss was 43.8528.\n",
            "Beginning training for epoch 36 of 50...\n",
            "After 36 epochs, test accuracy was 0.34. Final batch loss was 43.8421.\n",
            "Beginning training for epoch 37 of 50...\n",
            "After 37 epochs, test accuracy was 0.34. Final batch loss was 43.8459.\n",
            "Beginning training for epoch 38 of 50...\n",
            "After 38 epochs, test accuracy was 0.34. Final batch loss was 43.8429.\n",
            "Beginning training for epoch 39 of 50...\n",
            "After 39 epochs, test accuracy was 0.34. Final batch loss was 43.8345.\n",
            "Beginning training for epoch 40 of 50...\n",
            "After 40 epochs, test accuracy was 0.34. Final batch loss was 43.8263.\n",
            "Beginning training for epoch 41 of 50...\n",
            "After 41 epochs, test accuracy was 0.34. Final batch loss was 43.8270.\n",
            "Beginning training for epoch 42 of 50...\n",
            "After 42 epochs, test accuracy was 0.34. Final batch loss was 43.8430.\n",
            "Beginning training for epoch 43 of 50...\n",
            "After 43 epochs, test accuracy was 0.34. Final batch loss was 43.8308.\n",
            "Beginning training for epoch 44 of 50...\n",
            "After 44 epochs, test accuracy was 0.34. Final batch loss was 43.8238.\n",
            "Beginning training for epoch 45 of 50...\n",
            "After 45 epochs, test accuracy was 0.34. Final batch loss was 43.7885.\n",
            "Beginning training for epoch 46 of 50...\n",
            "After 46 epochs, test accuracy was 0.34. Final batch loss was 43.8123.\n",
            "Beginning training for epoch 47 of 50...\n",
            "After 47 epochs, test accuracy was 0.34. Final batch loss was 43.8103.\n",
            "Beginning training for epoch 48 of 50...\n",
            "After 48 epochs, test accuracy was 0.34. Final batch loss was 43.7975.\n",
            "Beginning training for epoch 49 of 50...\n",
            "After 49 epochs, test accuracy was 0.34. Final batch loss was 43.7932.\n",
            "Beginning training for epoch 50 of 50...\n",
            "After 50 epochs, test accuracy was 0.34. Final batch loss was 43.7783.\n"
          ]
        }
      ],
      "source": [
        "# Solve it with raw Jax using just node features. Using only node features (no graph structure)\n",
        "# is not expected to work well since we specifically designed the toy problem to need information\n",
        "# about the local neighborhood in order to make correct predictions.\n",
        "set_seeds(2)\n",
        "\n",
        "def init_mlp(layer_dimensions: list[int], parent_random_key: jax.random.PRNGKey, scale: float=.1) -> list[list[jnp.ndarray]]:\n",
        "  params = []\n",
        "  keys = jax.random.split(parent_random_key, num=len(layer_dimensions)-1)\n",
        "  for index, _ in enumerate(layer_dimensions[:-1]):\n",
        "    in_width = layer_dimensions[index]\n",
        "    out_width = layer_dimensions[index+1]\n",
        "    weight_key, bias_key = jax.random.split(keys[index])\n",
        "    params.append([\n",
        "      scale * jax.random.normal(weight_key, shape=(in_width, out_width)),\n",
        "      scale * jax.random.normal(bias_key, shape=(out_width,)),\n",
        "    ])\n",
        "  return params\n",
        "\n",
        "@jax.jit\n",
        "def forward(params: list[list[jnp.ndarray]], inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "  starting_vector = inputs\n",
        "  for index, layer in enumerate(params):\n",
        "    starting_vector = jnp.dot(starting_vector, layer[0]) + layer[1]\n",
        "    if index < len(params)-1:\n",
        "      # Don't RELU the final values.\n",
        "      starting_vector = jax.nn.relu(starting_vector)\n",
        "  return starting_vector\n",
        "\n",
        "forward_batched = jax.jit(jax.vmap(forward, in_axes=(None, 0)))\n",
        "\n",
        "@jax.jit\n",
        "def loss_function_single(params: list[list[jnp.ndarray]], inputs: jnp.ndarray, correct_labels: jnp.ndarray) -> float:\n",
        "  # inputs is shape (node_feature_vector_len,)\n",
        "  # correct_labels shape is (num_classes,)\n",
        "  predicted = forward(params, inputs)  # (num_classes,)\n",
        "  return optax.softmax_cross_entropy(predicted, correct_labels)\n",
        "\n",
        "@jax.jit\n",
        "def loss_function_batched(params: list[list[jnp.ndarray]], inputs: jnp.ndarray, correct_labels: jnp.ndarray) -> float:\n",
        "  # inputs is now shape (batch_size, node_feature_vector_len)\n",
        "  # correct_labels is now shape (batch_size, num_classes)\n",
        "  predicted = forward_batched(params, inputs)  # (batch_size, num_classes)\n",
        "  return jnp.sum(optax.softmax_cross_entropy(predicted, correct_labels))\n",
        "\n",
        "def calculate_accuracy_over_test_set_jax(params: list[list[jnp.ndarray]], test_set: jnp.ndarray, test_labels: jnp.ndarray) -> float:\n",
        "  # Returns the percentage of test examples classified correctly.\n",
        "  prediction_logits = forward(params, test_set)\n",
        "  softmaxed = jax.nn.softmax(prediction_logits)\n",
        "  predictions_argmax = jnp.argmax(softmaxed, axis=1)\n",
        "  ground_truth_argmax = jnp.argmax(test_labels, axis=1)\n",
        "  correct_count = jnp.sum(predictions_argmax == ground_truth_argmax)\n",
        "  return correct_count / test_labels.shape[0]\n",
        "\n",
        "\n",
        "seed = 12\n",
        "num_epochs = 50\n",
        "learning_rate = .001\n",
        "network_params = init_mlp(\n",
        "    [example_node.get_feature_vector(node_feature_a_size=node_feature_a_size).shape[0], 128, 128, num_classes],\n",
        "    jax.random.PRNGKey(seed),\n",
        ")\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Beginning training for epoch {epoch+1} of {num_epochs}...\")\n",
        "  for i in range(batched_train_set.shape[0]):\n",
        "    train_batch = batched_train_set[i]  # (batch_size, node_feature_vector_len)\n",
        "    train_labels = batched_train_labels[i]  # (batch_size, num_classes)\n",
        "    shuffled_indices = jax.random.permutation(jax.random.PRNGKey(epoch), jnp.arange(train_batch.shape[0]))\n",
        "    train_batch = train_batch[shuffled_indices]\n",
        "    train_labels = train_labels[shuffled_indices]\n",
        "    loss_value, loss_gradient = jax.value_and_grad(loss_function_batched)(network_params, train_batch, train_labels)\n",
        "    network_params = jax.tree.map(lambda p, g: p - learning_rate*g, network_params, loss_gradient)\n",
        "  accuracy = calculate_accuracy_over_test_set_jax(network_params, vectorized_test_set, vectorized_test_labels)\n",
        "  print(f\"After {epoch+1} epochs, test accuracy was {accuracy:.2f}. Final batch loss was {loss_value:.4f}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1OFVlsuFhmJ",
        "outputId": "a6c86251-13b6-4695-ac5c-41189979e16f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Param count 19208\n",
            "Beginning training for epoch 1 of 50...\n",
            "After 1 epochs, test accuracy was 0.33. Final batch loss was 1.5793.\n",
            "Beginning training for epoch 2 of 50...\n",
            "After 2 epochs, test accuracy was 0.34. Final batch loss was 1.5649.\n",
            "Beginning training for epoch 3 of 50...\n",
            "After 3 epochs, test accuracy was 0.34. Final batch loss was 1.5515.\n",
            "Beginning training for epoch 4 of 50...\n",
            "After 4 epochs, test accuracy was 0.34. Final batch loss was 1.5382.\n",
            "Beginning training for epoch 5 of 50...\n",
            "After 5 epochs, test accuracy was 0.34. Final batch loss was 1.5351.\n",
            "Beginning training for epoch 6 of 50...\n",
            "After 6 epochs, test accuracy was 0.34. Final batch loss was 1.5322.\n",
            "Beginning training for epoch 7 of 50...\n",
            "After 7 epochs, test accuracy was 0.34. Final batch loss was 1.5225.\n",
            "Beginning training for epoch 8 of 50...\n",
            "After 8 epochs, test accuracy was 0.34. Final batch loss was 1.5143.\n",
            "Beginning training for epoch 9 of 50...\n",
            "After 9 epochs, test accuracy was 0.34. Final batch loss was 1.5087.\n",
            "Beginning training for epoch 10 of 50...\n",
            "After 10 epochs, test accuracy was 0.34. Final batch loss was 1.5049.\n",
            "Beginning training for epoch 11 of 50...\n",
            "After 11 epochs, test accuracy was 0.34. Final batch loss was 1.4938.\n",
            "Beginning training for epoch 12 of 50...\n",
            "After 12 epochs, test accuracy was 0.34. Final batch loss was 1.4782.\n",
            "Beginning training for epoch 13 of 50...\n",
            "After 13 epochs, test accuracy was 0.34. Final batch loss was 1.4551.\n",
            "Beginning training for epoch 14 of 50...\n",
            "After 14 epochs, test accuracy was 0.34. Final batch loss was 1.4296.\n",
            "Beginning training for epoch 15 of 50...\n",
            "After 15 epochs, test accuracy was 0.34. Final batch loss was 1.4026.\n",
            "Beginning training for epoch 16 of 50...\n",
            "After 16 epochs, test accuracy was 0.34. Final batch loss was 1.3781.\n",
            "Beginning training for epoch 17 of 50...\n",
            "After 17 epochs, test accuracy was 0.34. Final batch loss was 1.3551.\n",
            "Beginning training for epoch 18 of 50...\n",
            "After 18 epochs, test accuracy was 0.34. Final batch loss was 1.3123.\n",
            "Beginning training for epoch 19 of 50...\n",
            "After 19 epochs, test accuracy was 0.34. Final batch loss was 1.2859.\n",
            "Beginning training for epoch 20 of 50...\n",
            "After 20 epochs, test accuracy was 0.34. Final batch loss was 1.2447.\n",
            "Beginning training for epoch 21 of 50...\n",
            "After 21 epochs, test accuracy was 0.34. Final batch loss was 1.1938.\n",
            "Beginning training for epoch 22 of 50...\n",
            "After 22 epochs, test accuracy was 0.34. Final batch loss was 1.1603.\n",
            "Beginning training for epoch 23 of 50...\n",
            "After 23 epochs, test accuracy was 0.34. Final batch loss was 1.1189.\n",
            "Beginning training for epoch 24 of 50...\n",
            "After 24 epochs, test accuracy was 0.34. Final batch loss was 1.0587.\n",
            "Beginning training for epoch 25 of 50...\n",
            "After 25 epochs, test accuracy was 0.34. Final batch loss was 1.0306.\n",
            "Beginning training for epoch 26 of 50...\n",
            "After 26 epochs, test accuracy was 0.34. Final batch loss was 0.9904.\n",
            "Beginning training for epoch 27 of 50...\n",
            "After 27 epochs, test accuracy was 0.34. Final batch loss was 0.9548.\n",
            "Beginning training for epoch 28 of 50...\n",
            "After 28 epochs, test accuracy was 0.34. Final batch loss was 0.9140.\n",
            "Beginning training for epoch 29 of 50...\n",
            "After 29 epochs, test accuracy was 0.34. Final batch loss was 0.8860.\n",
            "Beginning training for epoch 30 of 50...\n",
            "After 30 epochs, test accuracy was 0.34. Final batch loss was 0.8557.\n",
            "Beginning training for epoch 31 of 50...\n",
            "After 31 epochs, test accuracy was 0.34. Final batch loss was 0.8263.\n",
            "Beginning training for epoch 32 of 50...\n",
            "After 32 epochs, test accuracy was 0.34. Final batch loss was 0.8044.\n",
            "Beginning training for epoch 33 of 50...\n",
            "After 33 epochs, test accuracy was 0.34. Final batch loss was 0.7876.\n",
            "Beginning training for epoch 34 of 50...\n",
            "After 34 epochs, test accuracy was 0.34. Final batch loss was 0.7725.\n",
            "Beginning training for epoch 35 of 50...\n",
            "After 35 epochs, test accuracy was 0.34. Final batch loss was 0.7581.\n",
            "Beginning training for epoch 36 of 50...\n",
            "After 36 epochs, test accuracy was 0.34. Final batch loss was 0.7477.\n",
            "Beginning training for epoch 37 of 50...\n",
            "After 37 epochs, test accuracy was 0.34. Final batch loss was 0.7516.\n",
            "Beginning training for epoch 38 of 50...\n",
            "After 38 epochs, test accuracy was 0.34. Final batch loss was 0.7454.\n",
            "Beginning training for epoch 39 of 50...\n",
            "After 39 epochs, test accuracy was 0.34. Final batch loss was 0.7417.\n",
            "Beginning training for epoch 40 of 50...\n",
            "After 40 epochs, test accuracy was 0.34. Final batch loss was 0.7416.\n",
            "Beginning training for epoch 41 of 50...\n",
            "After 41 epochs, test accuracy was 0.34. Final batch loss was 0.7371.\n",
            "Beginning training for epoch 42 of 50...\n",
            "After 42 epochs, test accuracy was 0.34. Final batch loss was 0.7327.\n",
            "Beginning training for epoch 43 of 50...\n",
            "After 43 epochs, test accuracy was 0.34. Final batch loss was 0.7303.\n",
            "Beginning training for epoch 44 of 50...\n",
            "After 44 epochs, test accuracy was 0.34. Final batch loss was 0.7210.\n",
            "Beginning training for epoch 45 of 50...\n",
            "After 45 epochs, test accuracy was 0.34. Final batch loss was 0.7188.\n",
            "Beginning training for epoch 46 of 50...\n",
            "After 46 epochs, test accuracy was 0.34. Final batch loss was 0.7147.\n",
            "Beginning training for epoch 47 of 50...\n",
            "After 47 epochs, test accuracy was 0.34. Final batch loss was 0.7147.\n",
            "Beginning training for epoch 48 of 50...\n",
            "After 48 epochs, test accuracy was 0.34. Final batch loss was 0.7120.\n",
            "Beginning training for epoch 49 of 50...\n",
            "After 49 epochs, test accuracy was 0.34. Final batch loss was 0.7091.\n",
            "Beginning training for epoch 50 of 50...\n",
            "After 50 epochs, test accuracy was 0.34. Final batch loss was 0.7131.\n"
          ]
        }
      ],
      "source": [
        "# Solve it in PyTorch using just node features.\n",
        "set_seeds(2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class SimpleMlp(nn.Module):\n",
        "  \"\"\"Basic MLP to attempt to classify node features.\"\"\"\n",
        "\n",
        "  def __init__(self, *, node_feature_vector_len: int, num_classes: int):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Linear(node_feature_vector_len, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, num_classes),\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    return self.network(x)\n",
        "\n",
        "def calculate_accuracy_over_test_set_pytorch(params: SimpleMlp, test_set: torch.Tensor, test_labels: torch.Tensor) -> float:\n",
        "  # Returns the percentage of test examples classified correctly.\n",
        "  prediction_logits = params.forward(test_set)\n",
        "  softmaxed = nn.functional.softmax(prediction_logits, dim=1)\n",
        "  predictions_argmax = torch.argmax(softmaxed, dim=1)\n",
        "  ground_truth_argmax = torch.argmax(test_labels, dim=1)\n",
        "  correct_count = torch.sum(predictions_argmax == ground_truth_argmax)\n",
        "  return correct_count / test_labels.shape[0]\n",
        "\n",
        "\n",
        "mlp = SimpleMlp(node_feature_vector_len=example_node.get_feature_vector(node_feature_a_size=node_feature_a_size).shape[0], num_classes=num_classes).to(device)\n",
        "print(f\"Param count {sum(param.numel() for param in mlp.parameters())}\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=.001)\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Beginning training for epoch {epoch+1} of {num_epochs}...\")\n",
        "  for i in range(batched_train_set_torch.shape[0]):\n",
        "    train_batch = batched_train_set_torch[i]  # (batch_size, node_feature_vector_len)\n",
        "    train_labels = torch.argmax(batched_train_labels_torch[i], dim=1)  # (batch_size,)\n",
        "    num_rows = train_batch.shape[0]\n",
        "    shuffled_indices = torch.randperm(num_rows)\n",
        "    train_batch = train_batch[shuffled_indices]\n",
        "    train_labels = train_labels[shuffled_indices]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mlp.forward(train_batch)  # (batch_size, num_classes)\n",
        "    loss = criterion(outputs, train_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  accuracy = calculate_accuracy_over_test_set_pytorch(mlp, vectorized_test_set_torch, vectorized_test_labels_torch)\n",
        "  print(f\"After {epoch+1} epochs, test accuracy was {accuracy:.2f}. Final batch loss was {loss:.4f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cvy_LxQx30y"
      },
      "source": [
        "### Solve using GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4B6Nww1xSaU",
        "outputId": "a3711e82-1f9b-4945-d510-4dc0d6df578c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning training for epoch 1 of 50...\n",
            "After 1 epochs, test accuracy was 0.60. Final batch loss was 9.1806.\n",
            "Beginning training for epoch 2 of 50...\n",
            "After 2 epochs, test accuracy was 0.67. Final batch loss was 3.7298.\n",
            "Beginning training for epoch 3 of 50...\n",
            "After 3 epochs, test accuracy was 0.69. Final batch loss was 6.0752.\n",
            "Beginning training for epoch 4 of 50...\n",
            "After 4 epochs, test accuracy was 0.67. Final batch loss was 0.7907.\n",
            "Beginning training for epoch 5 of 50...\n",
            "After 5 epochs, test accuracy was 0.73. Final batch loss was 3.8979.\n",
            "Beginning training for epoch 6 of 50...\n",
            "After 6 epochs, test accuracy was 0.76. Final batch loss was 3.5668.\n",
            "Beginning training for epoch 7 of 50...\n",
            "After 7 epochs, test accuracy was 0.77. Final batch loss was 3.7210.\n",
            "Beginning training for epoch 8 of 50...\n",
            "After 8 epochs, test accuracy was 0.71. Final batch loss was 3.7890.\n",
            "Beginning training for epoch 9 of 50...\n",
            "After 9 epochs, test accuracy was 0.72. Final batch loss was 6.0502.\n",
            "Beginning training for epoch 10 of 50...\n",
            "After 10 epochs, test accuracy was 0.77. Final batch loss was 1.2725.\n",
            "Beginning training for epoch 11 of 50...\n",
            "After 11 epochs, test accuracy was 0.78. Final batch loss was 2.7420.\n",
            "Beginning training for epoch 12 of 50...\n",
            "After 12 epochs, test accuracy was 0.76. Final batch loss was 10.1801.\n",
            "Beginning training for epoch 13 of 50...\n",
            "After 13 epochs, test accuracy was 0.77. Final batch loss was 1.3669.\n",
            "Beginning training for epoch 14 of 50...\n",
            "After 14 epochs, test accuracy was 0.77. Final batch loss was 2.5357.\n",
            "Beginning training for epoch 15 of 50...\n",
            "After 15 epochs, test accuracy was 0.72. Final batch loss was 5.0917.\n",
            "Beginning training for epoch 16 of 50...\n",
            "After 16 epochs, test accuracy was 0.78. Final batch loss was 2.8787.\n",
            "Beginning training for epoch 17 of 50...\n",
            "After 17 epochs, test accuracy was 0.77. Final batch loss was 0.4172.\n",
            "Beginning training for epoch 18 of 50...\n",
            "After 18 epochs, test accuracy was 0.78. Final batch loss was 5.5576.\n",
            "Beginning training for epoch 19 of 50...\n",
            "After 19 epochs, test accuracy was 0.75. Final batch loss was 2.4259.\n",
            "Beginning training for epoch 20 of 50...\n",
            "After 20 epochs, test accuracy was 0.78. Final batch loss was 0.9808.\n",
            "Beginning training for epoch 21 of 50...\n",
            "After 21 epochs, test accuracy was 0.80. Final batch loss was 2.3279.\n",
            "Beginning training for epoch 22 of 50...\n",
            "After 22 epochs, test accuracy was 0.78. Final batch loss was 5.1646.\n",
            "Beginning training for epoch 23 of 50...\n",
            "After 23 epochs, test accuracy was 0.78. Final batch loss was 0.5179.\n",
            "Beginning training for epoch 24 of 50...\n",
            "After 24 epochs, test accuracy was 0.79. Final batch loss was 2.5266.\n",
            "Beginning training for epoch 25 of 50...\n",
            "After 25 epochs, test accuracy was 0.79. Final batch loss was 3.5802.\n",
            "Beginning training for epoch 26 of 50...\n",
            "After 26 epochs, test accuracy was 0.78. Final batch loss was 4.6625.\n",
            "Beginning training for epoch 27 of 50...\n",
            "After 27 epochs, test accuracy was 0.79. Final batch loss was 0.2071.\n",
            "Beginning training for epoch 28 of 50...\n",
            "After 28 epochs, test accuracy was 0.80. Final batch loss was 0.1906.\n",
            "Beginning training for epoch 29 of 50...\n",
            "After 29 epochs, test accuracy was 0.80. Final batch loss was 4.5178.\n",
            "Beginning training for epoch 30 of 50...\n",
            "After 30 epochs, test accuracy was 0.82. Final batch loss was 1.1143.\n",
            "Beginning training for epoch 31 of 50...\n",
            "After 31 epochs, test accuracy was 0.78. Final batch loss was 3.3855.\n",
            "Beginning training for epoch 32 of 50...\n",
            "After 32 epochs, test accuracy was 0.80. Final batch loss was 15.2811.\n",
            "Beginning training for epoch 33 of 50...\n",
            "After 33 epochs, test accuracy was 0.80. Final batch loss was 7.7055.\n",
            "Beginning training for epoch 34 of 50...\n",
            "After 34 epochs, test accuracy was 0.80. Final batch loss was 1.1444.\n",
            "Beginning training for epoch 35 of 50...\n",
            "After 35 epochs, test accuracy was 0.80. Final batch loss was 9.7951.\n",
            "Beginning training for epoch 36 of 50...\n",
            "After 36 epochs, test accuracy was 0.81. Final batch loss was 2.9021.\n",
            "Beginning training for epoch 37 of 50...\n",
            "After 37 epochs, test accuracy was 0.80. Final batch loss was 9.1642.\n",
            "Beginning training for epoch 38 of 50...\n",
            "After 38 epochs, test accuracy was 0.82. Final batch loss was 2.5414.\n",
            "Beginning training for epoch 39 of 50...\n",
            "After 39 epochs, test accuracy was 0.81. Final batch loss was 4.1939.\n",
            "Beginning training for epoch 40 of 50...\n",
            "After 40 epochs, test accuracy was 0.82. Final batch loss was 1.8618.\n",
            "Beginning training for epoch 41 of 50...\n",
            "After 41 epochs, test accuracy was 0.78. Final batch loss was 1.4999.\n",
            "Beginning training for epoch 42 of 50...\n",
            "After 42 epochs, test accuracy was 0.74. Final batch loss was 4.5025.\n",
            "Beginning training for epoch 43 of 50...\n",
            "After 43 epochs, test accuracy was 0.82. Final batch loss was 3.5906.\n",
            "Beginning training for epoch 44 of 50...\n",
            "After 44 epochs, test accuracy was 0.83. Final batch loss was 0.3853.\n",
            "Beginning training for epoch 45 of 50...\n",
            "After 45 epochs, test accuracy was 0.80. Final batch loss was 3.2247.\n",
            "Beginning training for epoch 46 of 50...\n",
            "After 46 epochs, test accuracy was 0.82. Final batch loss was 2.6467.\n",
            "Beginning training for epoch 47 of 50...\n",
            "After 47 epochs, test accuracy was 0.82. Final batch loss was 0.4015.\n",
            "Beginning training for epoch 48 of 50...\n",
            "After 48 epochs, test accuracy was 0.82. Final batch loss was 0.3526.\n",
            "Beginning training for epoch 49 of 50...\n",
            "After 49 epochs, test accuracy was 0.83. Final batch loss was 1.8914.\n",
            "Beginning training for epoch 50 of 50...\n",
            "After 50 epochs, test accuracy was 0.83. Final batch loss was 1.5234.\n"
          ]
        }
      ],
      "source": [
        "# GCN in Jax\n",
        "set_seeds(2)\n",
        "\n",
        "def init_gnn(*, node_feature_vector_len: int, num_classes: int, parent_random_key: jax.random.PRNGKey, scale: float=.1) -> dict[str, jnp.ndarray]:\n",
        "  params = {}\n",
        "  keys = jax.random.split(parent_random_key, num=12)\n",
        "\n",
        "  # Could do this in a loop but making everything super explicit since there are only 2 layers.\n",
        "  params[\"gnn_layer_1_w\"] = scale * jax.random.normal(keys[0], shape=(node_feature_vector_len, node_feature_vector_len))\n",
        "  params[\"gnn_layer_1_b\"] = scale * jax.random.normal(keys[1], shape=(node_feature_vector_len,))\n",
        "  params[\"self_update_1_w\"] = scale * jax.random.normal(keys[2], shape=(node_feature_vector_len, node_feature_vector_len))\n",
        "  params[\"self_update_1_b\"] = scale * jax.random.normal(keys[3], shape=(node_feature_vector_len,))\n",
        "  params[\"gnn_layer_2_w\"] = scale * jax.random.normal(keys[4], shape=(node_feature_vector_len, node_feature_vector_len))\n",
        "  params[\"gnn_layer_2_b\"] = scale * jax.random.normal(keys[5], shape=(node_feature_vector_len,))\n",
        "  params[\"self_update_2_w\"] = scale * jax.random.normal(keys[6], shape=(node_feature_vector_len, node_feature_vector_len))\n",
        "  params[\"self_update_2_b\"] = scale * jax.random.normal(keys[7], shape=(node_feature_vector_len,))\n",
        "  params[\"classification_head_mlp_1_w\"] = scale * jax.random.normal(keys[8], shape=(node_feature_vector_len, 128))\n",
        "  params[\"classification_head_mlp_1_b\"] = scale * jax.random.normal(keys[9], shape=(128,))\n",
        "  params[\"classification_head_mlp_2_w\"] = scale * jax.random.normal(keys[10], shape=(128, num_classes))\n",
        "  params[\"classification_head_mlp_2_b\"] = scale * jax.random.normal(keys[11], shape=(num_classes,))\n",
        "\n",
        "  return params\n",
        "\n",
        "@jax.jit\n",
        "def forward_batched(params: dict[str, jnp.ndarray], h0: jnp.ndarray, D_inv: jnp.ndarray, A: jnp.ndarray) -> jnp.ndarray:\n",
        "  # h0 is (V, d)\n",
        "  # D_inv is (V, V)\n",
        "  # A is (V, V)\n",
        "  d_inv_a = D_inv @ A  # (V, V)\n",
        "  gnn_layer_1 = (d_inv_a @ h0) @ params[\"gnn_layer_1_w\"] + params[\"gnn_layer_1_b\"]  # (V, d)\n",
        "  self_update_1 = (h0 @ params[\"self_update_1_w\"]) + params[\"self_update_1_b\"]  # (V, d)\n",
        "  h1 = jax.nn.relu(gnn_layer_1 + self_update_1)  # (V, d)\n",
        "  gnn_layer_2 = (d_inv_a @ h1) @ params[\"gnn_layer_2_w\"] + params[\"gnn_layer_2_b\"]  # (V, d)\n",
        "  self_update_2 = (h1 @ params[\"self_update_2_w\"]) + params[\"self_update_2_b\"]  # (V, d)\n",
        "  h2 = jax.nn.relu(gnn_layer_2 + self_update_2)  # (V, d)\n",
        "\n",
        "  mlp1 = (h2 @ params[\"classification_head_mlp_1_w\"]) + params[\"classification_head_mlp_1_b\"]  # (V, 128)\n",
        "  mlp1 = jax.nn.relu(mlp1)\n",
        "  return (mlp1 @ params[\"classification_head_mlp_2_w\"]) + params[\"classification_head_mlp_2_b\"]  # (V, num_classes)\n",
        "\n",
        "@jax.jit\n",
        "def loss_function_batched(\n",
        "    params: dict[str, jnp.ndarray],\n",
        "    h0: jnp.ndarray,\n",
        "    D_inv: jnp.ndarray,\n",
        "    A: jnp.ndarray,\n",
        "    correct_labels: jnp.ndarray\n",
        ") -> float:\n",
        "  # h0 is (V, d)\n",
        "  # D_inv is (V, V)\n",
        "  # A is (V, V)\n",
        "  # correct_labels is now shape (V, num_classes)\n",
        "  predicted = forward_batched(params, h0, D_inv, A)  # (V, num_classes)\n",
        "  return jnp.sum(optax.softmax_cross_entropy(predicted, correct_labels))\n",
        "\n",
        "def calculate_accuracy_over_test_set_jax(params: dict[str, jnp.ndarray], test_set: list[tuple[Graph, list[int]]]) -> float:\n",
        "  # Returns the percentage of test examples classified correctly.\n",
        "  correct_count = 0\n",
        "  total_count = 0\n",
        "  for graph, labels in test_set:\n",
        "    A = graph.get_adjacency_matrix_jnp(add_self_loops=False)\n",
        "    summed = jnp.sum(A, axis=1)\n",
        "    D_inv = jnp.diag(jnp.where(summed > 0, 1 / summed, 0))\n",
        "    all_node_feature_vectors = [\n",
        "        graph.id_to_node[i].get_feature_vector_jnp(node_feature_a_size=node_feature_a_size)\n",
        "        for i in range(len(graph.id_to_node))]\n",
        "    h0 = jnp.vstack(all_node_feature_vectors)\n",
        "    prediction_logits = forward_batched(params, h0, D_inv, A)\n",
        "    softmaxed = jax.nn.softmax(prediction_logits)\n",
        "    predictions_argmax = jnp.argmax(softmaxed, axis=1)\n",
        "    correct_count += jnp.sum(predictions_argmax == jnp.array(labels, dtype=jnp.int32))\n",
        "    total_count += len(labels)\n",
        "  return float(correct_count) / total_count\n",
        "\n",
        "\n",
        "num_epochs = 50\n",
        "learning_rate = .001\n",
        "network_params = init_gnn(\n",
        "    node_feature_vector_len=example_node.get_feature_vector(node_feature_a_size=node_feature_a_size).shape[0],\n",
        "    num_classes=num_classes,\n",
        "    parent_random_key=jax.random.PRNGKey(12),\n",
        ")\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Beginning training for epoch {epoch+1} of {num_epochs}...\")\n",
        "  train_set_copy = list(train_set)\n",
        "  random.shuffle(train_set_copy)\n",
        "  for graph, labels in train_set_copy:\n",
        "    A = graph.get_adjacency_matrix_jnp(add_self_loops=False)\n",
        "    summed = jnp.sum(A, axis=1)\n",
        "    D_inv = jnp.diag(jnp.where(summed > 0, 1 / summed, 0))\n",
        "    all_node_feature_vectors = [\n",
        "        graph.id_to_node[i].get_feature_vector_jnp(node_feature_a_size=node_feature_a_size)\n",
        "        for i in range(len(graph.id_to_node))]\n",
        "    h0 = jnp.vstack(all_node_feature_vectors)\n",
        "    targets = jnp.array(labels, dtype=jnp.int32)\n",
        "    one_hot_targets = jnp.eye(num_classes)[targets]\n",
        "    loss_value, loss_gradient = jax.value_and_grad(loss_function_batched)(network_params, h0, D_inv, A, one_hot_targets)\n",
        "    network_params = jax.tree.map(lambda p, g: p - learning_rate*g, network_params, loss_gradient)\n",
        "  accuracy = calculate_accuracy_over_test_set_jax(network_params, test_set)\n",
        "  print(f\"After {epoch+1} epochs, test accuracy was {accuracy:.2f}. Final batch loss was {loss_value:.4f}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku6t_7MWCbSa",
        "outputId": "792e9fac-a784-4026-92d2-e02849660316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Param count 3320\n",
            "Beginning training for epoch 1 of 50...\n",
            "After 1 epochs, test accuracy was 0.67. Final batch loss was 0.7736.\n",
            "Beginning training for epoch 2 of 50...\n",
            "After 2 epochs, test accuracy was 0.72. Final batch loss was 0.2663.\n",
            "Beginning training for epoch 3 of 50...\n",
            "After 3 epochs, test accuracy was 0.76. Final batch loss was 0.4433.\n",
            "Beginning training for epoch 4 of 50...\n",
            "After 4 epochs, test accuracy was 0.75. Final batch loss was 0.1447.\n",
            "Beginning training for epoch 5 of 50...\n",
            "After 5 epochs, test accuracy was 0.79. Final batch loss was 0.3416.\n",
            "Beginning training for epoch 6 of 50...\n",
            "After 6 epochs, test accuracy was 0.78. Final batch loss was 0.3632.\n",
            "Beginning training for epoch 7 of 50...\n",
            "After 7 epochs, test accuracy was 0.76. Final batch loss was 0.3755.\n",
            "Beginning training for epoch 8 of 50...\n",
            "After 8 epochs, test accuracy was 0.79. Final batch loss was 0.7155.\n",
            "Beginning training for epoch 9 of 50...\n",
            "After 9 epochs, test accuracy was 0.79. Final batch loss was 0.4404.\n",
            "Beginning training for epoch 10 of 50...\n",
            "After 10 epochs, test accuracy was 0.77. Final batch loss was 0.1970.\n",
            "Beginning training for epoch 11 of 50...\n",
            "After 11 epochs, test accuracy was 0.78. Final batch loss was 0.3404.\n",
            "Beginning training for epoch 12 of 50...\n",
            "After 12 epochs, test accuracy was 0.78. Final batch loss was 1.4889.\n",
            "Beginning training for epoch 13 of 50...\n",
            "After 13 epochs, test accuracy was 0.79. Final batch loss was 0.2245.\n",
            "Beginning training for epoch 14 of 50...\n",
            "After 14 epochs, test accuracy was 0.78. Final batch loss was 0.2416.\n",
            "Beginning training for epoch 15 of 50...\n",
            "After 15 epochs, test accuracy was 0.79. Final batch loss was 0.3413.\n",
            "Beginning training for epoch 16 of 50...\n",
            "After 16 epochs, test accuracy was 0.80. Final batch loss was 0.3589.\n",
            "Beginning training for epoch 17 of 50...\n",
            "After 17 epochs, test accuracy was 0.80. Final batch loss was 0.1200.\n",
            "Beginning training for epoch 18 of 50...\n",
            "After 18 epochs, test accuracy was 0.79. Final batch loss was 0.6356.\n",
            "Beginning training for epoch 19 of 50...\n",
            "After 19 epochs, test accuracy was 0.80. Final batch loss was 0.3576.\n",
            "Beginning training for epoch 20 of 50...\n",
            "After 20 epochs, test accuracy was 0.78. Final batch loss was 0.1606.\n",
            "Beginning training for epoch 21 of 50...\n",
            "After 21 epochs, test accuracy was 0.82. Final batch loss was 0.1925.\n",
            "Beginning training for epoch 22 of 50...\n",
            "After 22 epochs, test accuracy was 0.82. Final batch loss was 0.7292.\n",
            "Beginning training for epoch 23 of 50...\n",
            "After 23 epochs, test accuracy was 0.82. Final batch loss was 0.1312.\n",
            "Beginning training for epoch 24 of 50...\n",
            "After 24 epochs, test accuracy was 0.82. Final batch loss was 0.1743.\n",
            "Beginning training for epoch 25 of 50...\n",
            "After 25 epochs, test accuracy was 0.84. Final batch loss was 0.2061.\n",
            "Beginning training for epoch 26 of 50...\n",
            "After 26 epochs, test accuracy was 0.84. Final batch loss was 0.4014.\n",
            "Beginning training for epoch 27 of 50...\n",
            "After 27 epochs, test accuracy was 0.83. Final batch loss was 0.0965.\n",
            "Beginning training for epoch 28 of 50...\n",
            "After 28 epochs, test accuracy was 0.83. Final batch loss was 0.1121.\n",
            "Beginning training for epoch 29 of 50...\n",
            "After 29 epochs, test accuracy was 0.82. Final batch loss was 0.4341.\n",
            "Beginning training for epoch 30 of 50...\n",
            "After 30 epochs, test accuracy was 0.85. Final batch loss was 0.2105.\n",
            "Beginning training for epoch 31 of 50...\n",
            "After 31 epochs, test accuracy was 0.83. Final batch loss was 0.4268.\n",
            "Beginning training for epoch 32 of 50...\n",
            "After 32 epochs, test accuracy was 0.85. Final batch loss was 0.7381.\n",
            "Beginning training for epoch 33 of 50...\n",
            "After 33 epochs, test accuracy was 0.84. Final batch loss was 0.3645.\n",
            "Beginning training for epoch 34 of 50...\n",
            "After 34 epochs, test accuracy was 0.86. Final batch loss was 0.1215.\n",
            "Beginning training for epoch 35 of 50...\n",
            "After 35 epochs, test accuracy was 0.86. Final batch loss was 0.3961.\n",
            "Beginning training for epoch 36 of 50...\n",
            "After 36 epochs, test accuracy was 0.87. Final batch loss was 0.1598.\n",
            "Beginning training for epoch 37 of 50...\n",
            "After 37 epochs, test accuracy was 0.86. Final batch loss was 0.7232.\n",
            "Beginning training for epoch 38 of 50...\n",
            "After 38 epochs, test accuracy was 0.86. Final batch loss was 0.7326.\n",
            "Beginning training for epoch 39 of 50...\n",
            "After 39 epochs, test accuracy was 0.85. Final batch loss was 0.8799.\n",
            "Beginning training for epoch 40 of 50...\n",
            "After 40 epochs, test accuracy was 0.86. Final batch loss was 0.3047.\n",
            "Beginning training for epoch 41 of 50...\n",
            "After 41 epochs, test accuracy was 0.87. Final batch loss was 0.0818.\n",
            "Beginning training for epoch 42 of 50...\n",
            "After 42 epochs, test accuracy was 0.89. Final batch loss was 0.9509.\n",
            "Beginning training for epoch 43 of 50...\n",
            "After 43 epochs, test accuracy was 0.89. Final batch loss was 0.1185.\n",
            "Beginning training for epoch 44 of 50...\n",
            "After 44 epochs, test accuracy was 0.88. Final batch loss was 0.0221.\n",
            "Beginning training for epoch 45 of 50...\n",
            "After 45 epochs, test accuracy was 0.89. Final batch loss was 0.1258.\n",
            "Beginning training for epoch 46 of 50...\n",
            "After 46 epochs, test accuracy was 0.89. Final batch loss was 0.1853.\n",
            "Beginning training for epoch 47 of 50...\n",
            "After 47 epochs, test accuracy was 0.89. Final batch loss was 0.0075.\n",
            "Beginning training for epoch 48 of 50...\n",
            "After 48 epochs, test accuracy was 0.89. Final batch loss was 0.0139.\n",
            "Beginning training for epoch 49 of 50...\n",
            "After 49 epochs, test accuracy was 0.88. Final batch loss was 0.0877.\n",
            "Beginning training for epoch 50 of 50...\n",
            "After 50 epochs, test accuracy was 0.89. Final batch loss was 0.0292.\n"
          ]
        }
      ],
      "source": [
        "# GCN in PyTorch\n",
        "set_seeds(2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  \"\"\"Basic GCN to take graph structure into account when predicting.\"\"\"\n",
        "\n",
        "  def __init__(self, *, node_feature_vector_len: int, num_classes: int):\n",
        "    super().__init__()\n",
        "    self.gnn_layer_1 = nn.Linear(node_feature_vector_len, node_feature_vector_len)\n",
        "    self.self_update_1 = nn.Linear(node_feature_vector_len, node_feature_vector_len)\n",
        "    self.gnn_layer_2 = nn.Linear(node_feature_vector_len, node_feature_vector_len)\n",
        "    self.self_update_2 = nn.Linear(node_feature_vector_len, node_feature_vector_len)\n",
        "    self.classification_head = nn.Sequential(\n",
        "        nn.Linear(node_feature_vector_len, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, num_classes),\n",
        "    )\n",
        "\n",
        "  def forward(self, h0: torch.Tensor, D_inv: torch.Tensor, A: torch.Tensor) -> torch.Tensor:\n",
        "    # h0 is (V, d)\n",
        "    # D_inv is (V, V)\n",
        "    # A is (V, V)\n",
        "    h1 = nn.functional.relu(self.gnn_layer_1((D_inv @ A) @ h0) + self.self_update_1(h0))\n",
        "    h2 = nn.functional.relu(self.gnn_layer_2((D_inv @ A) @ h1) + self.self_update_2(h1))\n",
        "    return self.classification_head(h2)\n",
        "\n",
        "def calculate_accuracy_over_test_set_pytorch(params: GCN, test_set: list[tuple[Graph, list[int]]]) -> float:\n",
        "  # Returns the percentage of test examples classified correctly.\n",
        "  correct_count = 0\n",
        "  total_count = 0\n",
        "  for graph, labels in test_set:\n",
        "    A = torch.Tensor(graph.get_adjacency_matrix(add_self_loops=False)).to(device)\n",
        "    summed = torch.sum(A, dim=1).to(device)\n",
        "    D_inv = torch.diag(torch.where(summed > 0, 1 / summed, 0)).to(device)\n",
        "    all_node_feature_vectors = [\n",
        "        graph.id_to_node[i].get_feature_vector(node_feature_a_size=node_feature_a_size)\n",
        "        for i in range(len(graph.id_to_node))]\n",
        "    h0 = torch.Tensor(np.vstack(all_node_feature_vectors)).to(device)\n",
        "    prediction_logits = params.forward(h0, D_inv, A)\n",
        "    softmaxed = nn.functional.softmax(prediction_logits, dim=1)\n",
        "    predictions_argmax = torch.argmax(softmaxed, dim=1)\n",
        "    correct_count += torch.sum(predictions_argmax == torch.IntTensor(labels).to(device))\n",
        "    total_count += len(labels)\n",
        "  return float(correct_count) / total_count\n",
        "\n",
        "gcn = GCN(node_feature_vector_len=example_node.get_feature_vector(node_feature_a_size=node_feature_a_size).shape[0], num_classes=num_classes).to(device)\n",
        "print(f\"Param count {sum(param.numel() for param in gcn.parameters())}\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(gcn.parameters(), lr=.001)\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Beginning training for epoch {epoch+1} of {num_epochs}...\")\n",
        "  train_set_copy = list(train_set)\n",
        "  random.shuffle(train_set_copy)\n",
        "  for graph, labels in train_set_copy:\n",
        "    A = torch.Tensor(graph.get_adjacency_matrix(add_self_loops=False)).to(device)\n",
        "    summed = torch.sum(A, dim=1).to(device)\n",
        "    D_inv = torch.diag(torch.where(summed > 0, 1 / summed, 0)).to(device)\n",
        "    all_node_feature_vectors = [\n",
        "        graph.id_to_node[i].get_feature_vector(node_feature_a_size=node_feature_a_size)\n",
        "        for i in range(len(graph.id_to_node))]\n",
        "    h0 = torch.Tensor(np.vstack(all_node_feature_vectors)).to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = gcn.forward(h0, D_inv, A)  # (batch_size, num_classes)\n",
        "    loss = criterion(outputs, torch.LongTensor(labels).to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  accuracy = calculate_accuracy_over_test_set_pytorch(gcn, test_set)\n",
        "  print(f\"After {epoch+1} epochs, test accuracy was {accuracy:.2f}. Final batch loss was {loss:.4f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX5YIgYjPANp"
      },
      "source": [
        "### Solve using GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UccMJ_Xe7j1",
        "outputId": "c4341461-fbe1-48d0-d959-ec32f08a0d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning training for epoch 1 of 50...\n",
            "After 1 epochs, test accuracy was 0.63. Final batch loss was 7.3880.\n",
            "Beginning training for epoch 2 of 50...\n",
            "After 2 epochs, test accuracy was 0.68. Final batch loss was 3.8438.\n",
            "Beginning training for epoch 3 of 50...\n",
            "After 3 epochs, test accuracy was 0.62. Final batch loss was 5.5485.\n",
            "Beginning training for epoch 4 of 50...\n",
            "After 4 epochs, test accuracy was 0.64. Final batch loss was 0.7052.\n",
            "Beginning training for epoch 5 of 50...\n",
            "After 5 epochs, test accuracy was 0.74. Final batch loss was 2.7534.\n",
            "Beginning training for epoch 6 of 50...\n",
            "After 6 epochs, test accuracy was 0.76. Final batch loss was 6.0981.\n",
            "Beginning training for epoch 7 of 50...\n",
            "After 7 epochs, test accuracy was 0.76. Final batch loss was 3.5568.\n",
            "Beginning training for epoch 8 of 50...\n",
            "After 8 epochs, test accuracy was 0.74. Final batch loss was 4.3175.\n",
            "Beginning training for epoch 9 of 50...\n",
            "After 9 epochs, test accuracy was 0.70. Final batch loss was 4.6210.\n",
            "Beginning training for epoch 10 of 50...\n",
            "After 10 epochs, test accuracy was 0.76. Final batch loss was 0.7625.\n",
            "Beginning training for epoch 11 of 50...\n",
            "After 11 epochs, test accuracy was 0.81. Final batch loss was 5.3906.\n",
            "Beginning training for epoch 12 of 50...\n",
            "After 12 epochs, test accuracy was 0.72. Final batch loss was 7.4034.\n",
            "Beginning training for epoch 13 of 50...\n",
            "After 13 epochs, test accuracy was 0.77. Final batch loss was 1.7291.\n",
            "Beginning training for epoch 14 of 50...\n",
            "After 14 epochs, test accuracy was 0.77. Final batch loss was 3.1493.\n",
            "Beginning training for epoch 15 of 50...\n",
            "After 15 epochs, test accuracy was 0.77. Final batch loss was 5.6680.\n",
            "Beginning training for epoch 16 of 50...\n",
            "After 16 epochs, test accuracy was 0.80. Final batch loss was 3.1329.\n",
            "Beginning training for epoch 17 of 50...\n",
            "After 17 epochs, test accuracy was 0.70. Final batch loss was 1.2056.\n",
            "Beginning training for epoch 18 of 50...\n",
            "After 18 epochs, test accuracy was 0.81. Final batch loss was 7.0392.\n",
            "Beginning training for epoch 19 of 50...\n",
            "After 19 epochs, test accuracy was 0.74. Final batch loss was 4.1736.\n",
            "Beginning training for epoch 20 of 50...\n",
            "After 20 epochs, test accuracy was 0.78. Final batch loss was 1.7356.\n",
            "Beginning training for epoch 21 of 50...\n",
            "After 21 epochs, test accuracy was 0.81. Final batch loss was 3.6876.\n",
            "Beginning training for epoch 22 of 50...\n",
            "After 22 epochs, test accuracy was 0.75. Final batch loss was 4.1639.\n",
            "Beginning training for epoch 23 of 50...\n",
            "After 23 epochs, test accuracy was 0.80. Final batch loss was 0.7393.\n",
            "Beginning training for epoch 24 of 50...\n",
            "After 24 epochs, test accuracy was 0.84. Final batch loss was 1.9002.\n",
            "Beginning training for epoch 25 of 50...\n",
            "After 25 epochs, test accuracy was 0.78. Final batch loss was 1.2807.\n",
            "Beginning training for epoch 26 of 50...\n",
            "After 26 epochs, test accuracy was 0.84. Final batch loss was 6.0965.\n",
            "Beginning training for epoch 27 of 50...\n",
            "After 27 epochs, test accuracy was 0.84. Final batch loss was 0.2052.\n",
            "Beginning training for epoch 28 of 50...\n",
            "After 28 epochs, test accuracy was 0.59. Final batch loss was 5.6178.\n",
            "Beginning training for epoch 29 of 50...\n",
            "After 29 epochs, test accuracy was 0.78. Final batch loss was 2.5599.\n",
            "Beginning training for epoch 30 of 50...\n",
            "After 30 epochs, test accuracy was 0.86. Final batch loss was 1.7655.\n",
            "Beginning training for epoch 31 of 50...\n",
            "After 31 epochs, test accuracy was 0.84. Final batch loss was 2.9414.\n",
            "Beginning training for epoch 32 of 50...\n",
            "After 32 epochs, test accuracy was 0.79. Final batch loss was 7.3469.\n",
            "Beginning training for epoch 33 of 50...\n",
            "After 33 epochs, test accuracy was 0.76. Final batch loss was 5.2728.\n",
            "Beginning training for epoch 34 of 50...\n",
            "After 34 epochs, test accuracy was 0.84. Final batch loss was 2.2051.\n",
            "Beginning training for epoch 35 of 50...\n",
            "After 35 epochs, test accuracy was 0.86. Final batch loss was 1.8622.\n",
            "Beginning training for epoch 36 of 50...\n",
            "After 36 epochs, test accuracy was 0.84. Final batch loss was 1.3382.\n",
            "Beginning training for epoch 37 of 50...\n",
            "After 37 epochs, test accuracy was 0.77. Final batch loss was 14.2986.\n",
            "Beginning training for epoch 38 of 50...\n",
            "After 38 epochs, test accuracy was 0.86. Final batch loss was 1.3792.\n",
            "Beginning training for epoch 39 of 50...\n",
            "After 39 epochs, test accuracy was 0.86. Final batch loss was 5.2550.\n",
            "Beginning training for epoch 40 of 50...\n",
            "After 40 epochs, test accuracy was 0.87. Final batch loss was 1.3272.\n",
            "Beginning training for epoch 41 of 50...\n",
            "After 41 epochs, test accuracy was 0.81. Final batch loss was 2.5523.\n",
            "Beginning training for epoch 42 of 50...\n",
            "After 42 epochs, test accuracy was 0.79. Final batch loss was 7.3604.\n",
            "Beginning training for epoch 43 of 50...\n",
            "After 43 epochs, test accuracy was 0.85. Final batch loss was 1.7475.\n",
            "Beginning training for epoch 44 of 50...\n",
            "After 44 epochs, test accuracy was 0.86. Final batch loss was 0.2238.\n",
            "Beginning training for epoch 45 of 50...\n",
            "After 45 epochs, test accuracy was 0.85. Final batch loss was 0.9714.\n",
            "Beginning training for epoch 46 of 50...\n",
            "After 46 epochs, test accuracy was 0.84. Final batch loss was 2.2000.\n",
            "Beginning training for epoch 47 of 50...\n",
            "After 47 epochs, test accuracy was 0.86. Final batch loss was 1.6386.\n",
            "Beginning training for epoch 48 of 50...\n",
            "After 48 epochs, test accuracy was 0.85. Final batch loss was 0.4406.\n",
            "Beginning training for epoch 49 of 50...\n",
            "After 49 epochs, test accuracy was 0.78. Final batch loss was 2.7411.\n",
            "Beginning training for epoch 50 of 50...\n",
            "After 50 epochs, test accuracy was 0.84. Final batch loss was 0.9628.\n"
          ]
        }
      ],
      "source": [
        "# GAT in Jax\n",
        "set_seeds(2)\n",
        "\n",
        "def init_gnn(*, node_feature_vector_len: int, num_classes: int, parent_random_key: jax.random.PRNGKey, scale: float=.1) -> dict[str, jnp.ndarray]:\n",
        "  params = {}\n",
        "  keys = jax.random.split(parent_random_key, num=20)\n",
        "\n",
        "  # Could do this in a loop but making everything super explicit since there are only 2 layers.\n",
        "  params[\"gnn_layer_1_w\"] = scale * jax.random.normal(keys[0], shape=(node_feature_vector_len, node_feature_vector_len))\n",
        "  params[\"gnn_layer_1_b\"] = scale * jax.random.normal(keys[1], shape=(node_feature_vector_len,))\n",
        "  params[\"self_update_1_w\"] = scale * jax.random.normal(keys[2], shape=(node_feature_vector_len, node_feature_vector_len))\n",
        "  params[\"self_update_1_b\"] = scale * jax.random.normal(keys[3], shape=(node_feature_vector_len,))\n",
        "  params[\"layer_1_mha_head_1_w\"] = scale * jax.random.normal(keys[4], shape=(2 * node_feature_vector_len, 1))\n",
        "  params[\"layer_1_mha_head_1_b\"] = scale * jax.random.normal(keys[5], shape=(1,))\n",
        "  params[\"layer_1_mha_head_2_w\"] = scale * jax.random.normal(keys[6], shape=(2 * node_feature_vector_len, 1))\n",
        "  params[\"layer_1_mha_head_2_b\"] = scale * jax.random.normal(keys[7], shape=(1,))\n",
        "  params[\"gnn_layer_2_w\"] = scale * jax.random.normal(keys[8], shape=(node_feature_vector_len, node_feature_vector_len))\n",
        "  params[\"gnn_layer_2_b\"] = scale * jax.random.normal(keys[9], shape=(node_feature_vector_len,))\n",
        "  params[\"self_update_2_w\"] = scale * jax.random.normal(keys[10], shape=(node_feature_vector_len, node_feature_vector_len))\n",
        "  params[\"self_update_2_b\"] = scale * jax.random.normal(keys[11], shape=(node_feature_vector_len,))\n",
        "  params[\"layer_2_mha_head_1_w\"] = scale * jax.random.normal(keys[12], shape=(2 * node_feature_vector_len, 1))\n",
        "  params[\"layer_2_mha_head_1_b\"] = scale * jax.random.normal(keys[13], shape=(1,))\n",
        "  params[\"layer_2_mha_head_2_w\"] = scale * jax.random.normal(keys[14], shape=(2 * node_feature_vector_len, 1))\n",
        "  params[\"layer_2_mha_head_2_b\"] = scale * jax.random.normal(keys[15], shape=(1,))\n",
        "  params[\"classification_head_mlp_1_w\"] = scale * jax.random.normal(keys[16], shape=(node_feature_vector_len, 128))\n",
        "  params[\"classification_head_mlp_1_b\"] = scale * jax.random.normal(keys[17], shape=(128,))\n",
        "  params[\"classification_head_mlp_2_w\"] = scale * jax.random.normal(keys[18], shape=(128, num_classes))\n",
        "  params[\"classification_head_mlp_2_b\"] = scale * jax.random.normal(keys[19], shape=(num_classes,))\n",
        "\n",
        "  return params\n",
        "\n",
        "@jax.jit\n",
        "def forward_batched(params: dict[str, jnp.ndarray], h0: jnp.ndarray, A: jnp.ndarray) -> jnp.ndarray:\n",
        "  # h0 is (V, d)\n",
        "  # A is (V, V)\n",
        "  starting_vals = (h0 @ params[\"gnn_layer_1_w\"]) + params[\"gnn_layer_1_b\"]  # (V, d)\n",
        "  lhs = jnp.repeat(starting_vals, starting_vals.shape[0], axis=0)  # (V^2, d)\n",
        "  rhs = jnp.tile(starting_vals, (starting_vals.shape[0], 1))  # (V^2, d)\n",
        "  ans = jnp.hstack([lhs, rhs])  # (V^2, 2d)\n",
        "  attn_logits_head_1 = (ans @ params[\"layer_1_mha_head_1_w\"]) + params[\"layer_1_mha_head_1_b\"]  # (V^2, 1)\n",
        "  attn_logits_head_1 = jax.nn.relu(attn_logits_head_1).reshape((starting_vals.shape[0], starting_vals.shape[0]))  # (V, V)\n",
        "  attn_weights_layer_1_head_1 = jax.nn.softmax(attn_logits_head_1 * A, axis=1)  # (V, V)\n",
        "  attn_logits_head_2 = (ans @ params[\"layer_1_mha_head_2_w\"]) + params[\"layer_1_mha_head_2_b\"]  # (V^2, 1)\n",
        "  attn_logits_head_2 = jax.nn.relu(attn_logits_head_2).reshape((starting_vals.shape[0], starting_vals.shape[0]))  # (V, V)\n",
        "  attn_weights_layer_1_head_2 = jax.nn.softmax(attn_logits_head_2 * A, axis=1)  # (V, V)\n",
        "  self_update_1 = (h0 @ params[\"self_update_1_w\"]) + params[\"self_update_1_b\"]  # (V, d)\n",
        "  h1_head1 = jax.nn.relu((attn_weights_layer_1_head_1 @ starting_vals) + self_update_1)\n",
        "  h1_head2 = jax.nn.relu((attn_weights_layer_1_head_2 @ starting_vals) + self_update_1)\n",
        "  h1 = h1_head1 + h1_head2  # Sum pool\n",
        "\n",
        "  starting_vals = (h1 @ params[\"gnn_layer_2_w\"]) + params[\"gnn_layer_2_b\"]  # (V, d)\n",
        "  lhs = jnp.repeat(starting_vals, starting_vals.shape[0], axis=0)  # (V^2, d)\n",
        "  rhs = jnp.tile(starting_vals, (starting_vals.shape[0], 1))  # (V^2, d)\n",
        "  ans = jnp.hstack([lhs, rhs])  # (V^2, 2d)\n",
        "  attn_logits_head_1 = (ans @ params[\"layer_2_mha_head_1_w\"]) + params[\"layer_2_mha_head_1_b\"]  # (V^2, 1)\n",
        "  attn_logits_head_1 = jax.nn.relu(attn_logits_head_1).reshape((starting_vals.shape[0], starting_vals.shape[0]))  # (V, V)\n",
        "  attn_weights_layer_2_head_1 = jax.nn.softmax(attn_logits_head_1 * A, axis=1)  # (V, V)\n",
        "  attn_logits_head_2 = (ans @ params[\"layer_2_mha_head_2_w\"]) + params[\"layer_2_mha_head_2_b\"]  # (V^2, 1)\n",
        "  attn_logits_head_2 = jax.nn.relu(attn_logits_head_2).reshape((starting_vals.shape[0], starting_vals.shape[0]))  # (V, V)\n",
        "  attn_weights_layer_2_head_2 = jax.nn.softmax(attn_logits_head_2 * A, axis=1)  # (V, V)\n",
        "  self_update_2 = (h1 @ params[\"self_update_2_w\"]) + params[\"self_update_2_b\"]  # (V, d)\n",
        "  h2_head1 = jax.nn.relu((attn_weights_layer_2_head_1 @ starting_vals) + self_update_2)\n",
        "  h2_head2 = jax.nn.relu((attn_weights_layer_2_head_2 @ starting_vals) + self_update_2)\n",
        "  h2 = h2_head1 + h2_head2  # Sum pool\n",
        "\n",
        "  mlp1 = (h2 @ params[\"classification_head_mlp_1_w\"]) + params[\"classification_head_mlp_1_b\"]  # (V, 128)\n",
        "  mlp1 = jax.nn.relu(mlp1)\n",
        "  return (mlp1 @ params[\"classification_head_mlp_2_w\"]) + params[\"classification_head_mlp_2_b\"]  # (V, num_classes)\n",
        "\n",
        "@jax.jit\n",
        "def loss_function_batched(\n",
        "    params: dict[str, jnp.ndarray],\n",
        "    h0: jnp.ndarray,\n",
        "    A: jnp.ndarray,\n",
        "    correct_labels: jnp.ndarray\n",
        ") -> float:\n",
        "  # h0 is (V, d)\n",
        "  # A is (V, V)\n",
        "  # correct_labels is now shape (V, num_classes)\n",
        "  predicted = forward_batched(params, h0, A)  # (V, num_classes)\n",
        "  return jnp.sum(optax.softmax_cross_entropy(predicted, correct_labels))\n",
        "\n",
        "def calculate_accuracy_over_test_set_jax(params: dict[str, jnp.ndarray], test_set: list[tuple[Graph, list[int]]]) -> float:\n",
        "  # Returns the percentage of test examples classified correctly.\n",
        "  correct_count = 0\n",
        "  total_count = 0\n",
        "  for graph, labels in test_set:\n",
        "    A = graph.get_adjacency_matrix_jnp(add_self_loops=False)\n",
        "    all_node_feature_vectors = [\n",
        "        graph.id_to_node[i].get_feature_vector_jnp(node_feature_a_size=node_feature_a_size)\n",
        "        for i in range(len(graph.id_to_node))]\n",
        "    h0 = jnp.vstack(all_node_feature_vectors)\n",
        "    prediction_logits = forward_batched(params, h0, A)\n",
        "    softmaxed = jax.nn.softmax(prediction_logits)\n",
        "    predictions_argmax = jnp.argmax(softmaxed, axis=1)\n",
        "    correct_count += jnp.sum(predictions_argmax == jnp.array(labels, dtype=jnp.int32))\n",
        "    total_count += len(labels)\n",
        "  return float(correct_count) / total_count\n",
        "\n",
        "\n",
        "num_epochs = 50\n",
        "learning_rate = .001\n",
        "network_params = init_gnn(\n",
        "    node_feature_vector_len=example_node.get_feature_vector(node_feature_a_size=node_feature_a_size).shape[0],\n",
        "    num_classes=num_classes,\n",
        "    parent_random_key=jax.random.PRNGKey(12),\n",
        ")\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Beginning training for epoch {epoch+1} of {num_epochs}...\")\n",
        "  train_set_copy = list(train_set)\n",
        "  random.shuffle(train_set_copy)\n",
        "  for graph, labels in train_set_copy:\n",
        "    A = graph.get_adjacency_matrix_jnp(add_self_loops=False)\n",
        "    all_node_feature_vectors = [\n",
        "        graph.id_to_node[i].get_feature_vector_jnp(node_feature_a_size=node_feature_a_size)\n",
        "        for i in range(len(graph.id_to_node))]\n",
        "    h0 = jnp.vstack(all_node_feature_vectors)\n",
        "    targets = jnp.array(labels, dtype=jnp.int32)\n",
        "    one_hot_targets = jnp.eye(num_classes)[targets]\n",
        "    loss_value, loss_gradient = jax.value_and_grad(loss_function_batched)(network_params, h0, A, one_hot_targets)\n",
        "    network_params = jax.tree.map(lambda p, g: p - learning_rate*g, network_params, loss_gradient)\n",
        "  accuracy = calculate_accuracy_over_test_set_jax(network_params, test_set)\n",
        "  print(f\"After {epoch+1} epochs, test accuracy was {accuracy:.2f}. Final batch loss was {loss_value:.4f}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvuw-KoCPF7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5354f664-198e-4198-c358-5997fdd3c205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Param count 3420\n",
            "Beginning training for epoch 1 of 50...\n",
            "After 1 epochs, test accuracy was 0.64. Final batch loss was 0.5924.\n",
            "Beginning training for epoch 2 of 50...\n",
            "After 2 epochs, test accuracy was 0.65. Final batch loss was 0.4348.\n",
            "Beginning training for epoch 3 of 50...\n",
            "After 3 epochs, test accuracy was 0.71. Final batch loss was 0.4477.\n",
            "Beginning training for epoch 4 of 50...\n",
            "After 4 epochs, test accuracy was 0.71. Final batch loss was 0.1075.\n",
            "Beginning training for epoch 5 of 50...\n",
            "After 5 epochs, test accuracy was 0.73. Final batch loss was 0.2920.\n",
            "Beginning training for epoch 6 of 50...\n",
            "After 6 epochs, test accuracy was 0.75. Final batch loss was 0.3675.\n",
            "Beginning training for epoch 7 of 50...\n",
            "After 7 epochs, test accuracy was 0.77. Final batch loss was 0.4094.\n",
            "Beginning training for epoch 8 of 50...\n",
            "After 8 epochs, test accuracy was 0.75. Final batch loss was 0.6922.\n",
            "Beginning training for epoch 9 of 50...\n",
            "After 9 epochs, test accuracy was 0.79. Final batch loss was 0.4744.\n",
            "Beginning training for epoch 10 of 50...\n",
            "After 10 epochs, test accuracy was 0.71. Final batch loss was 0.1499.\n",
            "Beginning training for epoch 11 of 50...\n",
            "After 11 epochs, test accuracy was 0.78. Final batch loss was 0.5952.\n",
            "Beginning training for epoch 12 of 50...\n",
            "After 12 epochs, test accuracy was 0.79. Final batch loss was 1.6209.\n",
            "Beginning training for epoch 13 of 50...\n",
            "After 13 epochs, test accuracy was 0.79. Final batch loss was 0.2289.\n",
            "Beginning training for epoch 14 of 50...\n",
            "After 14 epochs, test accuracy was 0.77. Final batch loss was 0.1784.\n",
            "Beginning training for epoch 15 of 50...\n",
            "After 15 epochs, test accuracy was 0.77. Final batch loss was 0.2786.\n",
            "Beginning training for epoch 16 of 50...\n",
            "After 16 epochs, test accuracy was 0.80. Final batch loss was 0.3966.\n",
            "Beginning training for epoch 17 of 50...\n",
            "After 17 epochs, test accuracy was 0.80. Final batch loss was 0.0292.\n",
            "Beginning training for epoch 18 of 50...\n",
            "After 18 epochs, test accuracy was 0.80. Final batch loss was 0.7672.\n",
            "Beginning training for epoch 19 of 50...\n",
            "After 19 epochs, test accuracy was 0.81. Final batch loss was 0.3588.\n",
            "Beginning training for epoch 20 of 50...\n",
            "After 20 epochs, test accuracy was 0.80. Final batch loss was 0.2159.\n",
            "Beginning training for epoch 21 of 50...\n",
            "After 21 epochs, test accuracy was 0.81. Final batch loss was 0.1814.\n",
            "Beginning training for epoch 22 of 50...\n",
            "After 22 epochs, test accuracy was 0.83. Final batch loss was 0.5861.\n",
            "Beginning training for epoch 23 of 50...\n",
            "After 23 epochs, test accuracy was 0.82. Final batch loss was 0.0480.\n",
            "Beginning training for epoch 24 of 50...\n",
            "After 24 epochs, test accuracy was 0.85. Final batch loss was 0.2936.\n",
            "Beginning training for epoch 25 of 50...\n",
            "After 25 epochs, test accuracy was 0.83. Final batch loss was 0.2020.\n",
            "Beginning training for epoch 26 of 50...\n",
            "After 26 epochs, test accuracy was 0.84. Final batch loss was 0.4328.\n",
            "Beginning training for epoch 27 of 50...\n",
            "After 27 epochs, test accuracy was 0.83. Final batch loss was 0.0528.\n",
            "Beginning training for epoch 28 of 50...\n",
            "After 28 epochs, test accuracy was 0.84. Final batch loss was 0.0651.\n",
            "Beginning training for epoch 29 of 50...\n",
            "After 29 epochs, test accuracy was 0.84. Final batch loss was 0.3355.\n",
            "Beginning training for epoch 30 of 50...\n",
            "After 30 epochs, test accuracy was 0.84. Final batch loss was 0.1312.\n",
            "Beginning training for epoch 31 of 50...\n",
            "After 31 epochs, test accuracy was 0.84. Final batch loss was 0.6211.\n",
            "Beginning training for epoch 32 of 50...\n",
            "After 32 epochs, test accuracy was 0.84. Final batch loss was 0.4810.\n",
            "Beginning training for epoch 33 of 50...\n",
            "After 33 epochs, test accuracy was 0.83. Final batch loss was 0.4901.\n",
            "Beginning training for epoch 34 of 50...\n",
            "After 34 epochs, test accuracy was 0.83. Final batch loss was 0.1469.\n",
            "Beginning training for epoch 35 of 50...\n",
            "After 35 epochs, test accuracy was 0.85. Final batch loss was 0.6080.\n",
            "Beginning training for epoch 36 of 50...\n",
            "After 36 epochs, test accuracy was 0.85. Final batch loss was 0.1749.\n",
            "Beginning training for epoch 37 of 50...\n",
            "After 37 epochs, test accuracy was 0.77. Final batch loss was 0.6253.\n",
            "Beginning training for epoch 38 of 50...\n",
            "After 38 epochs, test accuracy was 0.83. Final batch loss was 0.6074.\n",
            "Beginning training for epoch 39 of 50...\n",
            "After 39 epochs, test accuracy was 0.84. Final batch loss was 0.3180.\n",
            "Beginning training for epoch 40 of 50...\n",
            "After 40 epochs, test accuracy was 0.84. Final batch loss was 0.2358.\n",
            "Beginning training for epoch 41 of 50...\n",
            "After 41 epochs, test accuracy was 0.86. Final batch loss was 0.2212.\n",
            "Beginning training for epoch 42 of 50...\n",
            "After 42 epochs, test accuracy was 0.85. Final batch loss was 1.7488.\n",
            "Beginning training for epoch 43 of 50...\n",
            "After 43 epochs, test accuracy was 0.86. Final batch loss was 0.4356.\n",
            "Beginning training for epoch 44 of 50...\n",
            "After 44 epochs, test accuracy was 0.85. Final batch loss was 0.0449.\n",
            "Beginning training for epoch 45 of 50...\n",
            "After 45 epochs, test accuracy was 0.82. Final batch loss was 0.4114.\n",
            "Beginning training for epoch 46 of 50...\n",
            "After 46 epochs, test accuracy was 0.85. Final batch loss was 0.2822.\n",
            "Beginning training for epoch 47 of 50...\n",
            "After 47 epochs, test accuracy was 0.86. Final batch loss was 0.0431.\n",
            "Beginning training for epoch 48 of 50...\n",
            "After 48 epochs, test accuracy was 0.84. Final batch loss was 0.1644.\n",
            "Beginning training for epoch 49 of 50...\n",
            "After 49 epochs, test accuracy was 0.82. Final batch loss was 0.1761.\n",
            "Beginning training for epoch 50 of 50...\n",
            "After 50 epochs, test accuracy was 0.86. Final batch loss was 0.1179.\n"
          ]
        }
      ],
      "source": [
        "# GAT in PyTorch\n",
        "set_seeds(2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class GAT(nn.Module):\n",
        "  \"\"\"Use attention to attend to different neighbors differently.\"\"\"\n",
        "\n",
        "  def __init__(self, *, node_feature_vector_len: int, num_classes: int):\n",
        "    super().__init__()\n",
        "    self.gnn_layer_1 = nn.Linear(node_feature_vector_len, node_feature_vector_len)\n",
        "    self.self_update_1 = nn.Linear(node_feature_vector_len, node_feature_vector_len)\n",
        "    self.layer_1_mha_head_1 = nn.Linear(2 * node_feature_vector_len, 1)\n",
        "    self.layer_1_mha_head_2 = nn.Linear(2 * node_feature_vector_len, 1)\n",
        "    self.gnn_layer_2 = nn.Linear(node_feature_vector_len, node_feature_vector_len)\n",
        "    self.self_update_2 = nn.Linear(node_feature_vector_len, node_feature_vector_len)\n",
        "    self.layer_2_mha_head_1 = nn.Linear(2 * node_feature_vector_len, 1)\n",
        "    self.layer_2_mha_head_2 = nn.Linear(2 * node_feature_vector_len, 1)\n",
        "    self.classification_head = nn.Sequential(\n",
        "        nn.Linear(node_feature_vector_len, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, num_classes),\n",
        "    )\n",
        "\n",
        "  def forward(self, h0: torch.Tensor, A: torch.Tensor) -> torch.Tensor:\n",
        "    # h0 is (V, d)\n",
        "    # A is (V, V)\n",
        "    # Realistically the MHA should be extracted to a module and use loops instead of copy/paste,\n",
        "    # but this is meant to be quick, in-line, and explicit.\n",
        "    starting_vals = self.gnn_layer_1(h0)  # (V, d)\n",
        "    lhs = torch.repeat_interleave(starting_vals, starting_vals.shape[0], dim=0)  # (V^2, d)\n",
        "    rhs = starting_vals.repeat(starting_vals.shape[0], 1)  # (V^2, d)\n",
        "    ans = torch.hstack([lhs, rhs])  # (V^2, 2d)\n",
        "    attn_logits_head_1 = self.layer_1_mha_head_1(ans)  # (V^2, 1)\n",
        "    attn_logits_head_1 = torch.nn.functional.relu(attn_logits_head_1).reshape((starting_vals.shape[0], starting_vals.shape[0]))  # (V, V)\n",
        "    attn_weights_layer_1_head_1 = torch.nn.functional.softmax(attn_logits_head_1 * A, dim=1)  # (V, V)\n",
        "    attn_logits_head_2 = self.layer_1_mha_head_2(ans)  # (V^2, 1)\n",
        "    attn_logits_head_2 = torch.nn.functional.relu(attn_logits_head_2).reshape((starting_vals.shape[0], starting_vals.shape[0]))  # (V, V)\n",
        "    attn_weights_layer_1_head_2 = torch.nn.functional.softmax(attn_logits_head_2 * A, dim=1)  # (V, V)\n",
        "    self_update = self.self_update_1(h0)\n",
        "    h1_head1 = nn.functional.relu((attn_weights_layer_1_head_1 @ starting_vals) + self_update)\n",
        "    h1_head2 = nn.functional.relu((attn_weights_layer_1_head_2 @ starting_vals) + self_update)\n",
        "    h1 = h1_head1 + h1_head2  # Sum pool\n",
        "\n",
        "    starting_vals = self.gnn_layer_2(h1)  # (V, d)\n",
        "    lhs = torch.repeat_interleave(starting_vals, starting_vals.shape[0], dim=0)  # (V^2, d)\n",
        "    rhs = starting_vals.repeat(starting_vals.shape[0], 1)  # (V^2, d)\n",
        "    ans = torch.hstack([lhs, rhs])  # (V^2, 2d)\n",
        "    attn_logits_head_1 = self.layer_2_mha_head_1(ans)  # (V^2, 1)\n",
        "    attn_logits_head_1 = torch.nn.functional.relu(attn_logits_head_1).reshape((starting_vals.shape[0], starting_vals.shape[0]))  # (V, V)\n",
        "    attn_weights_layer_2_head_1 = torch.nn.functional.softmax(attn_logits_head_1 * A, dim=1)  # (V, V)\n",
        "    attn_logits_head_2 = self.layer_2_mha_head_2(ans)  # (V^2, 1)\n",
        "    attn_logits_head_2 = torch.nn.functional.relu(attn_logits_head_2).reshape((starting_vals.shape[0], starting_vals.shape[0]))  # (V, V)\n",
        "    attn_weights_layer_2_head_2 = torch.nn.functional.softmax(attn_logits_head_2 * A, dim=1)  # (V, V)\n",
        "    self_update = self.self_update_2(h1)\n",
        "    h2_head1 = nn.functional.relu((attn_weights_layer_2_head_1 @ starting_vals) + self_update)\n",
        "    h2_head2 = nn.functional.relu((attn_weights_layer_2_head_2 @ starting_vals) + self_update)\n",
        "    h2 = h2_head1 + h2_head2  # Sum pool\n",
        "\n",
        "    return self.classification_head(h2)\n",
        "\n",
        "def calculate_accuracy_over_test_set_pytorch(params: GAT, test_set: list[tuple[Graph, list[int]]]) -> float:\n",
        "  # Returns the percentage of test examples classified correctly.\n",
        "  correct_count = 0\n",
        "  total_count = 0\n",
        "  for graph, labels in test_set:\n",
        "    A = torch.Tensor(graph.get_adjacency_matrix(add_self_loops=False)).to(device)\n",
        "    all_node_feature_vectors = [\n",
        "        graph.id_to_node[i].get_feature_vector(node_feature_a_size=node_feature_a_size)\n",
        "        for i in range(len(graph.id_to_node))]\n",
        "    h0 = torch.Tensor(np.vstack(all_node_feature_vectors)).to(device)\n",
        "    prediction_logits = params.forward(h0, A)\n",
        "    softmaxed = nn.functional.softmax(prediction_logits, dim=1)\n",
        "    predictions_argmax = torch.argmax(softmaxed, dim=1)\n",
        "    correct_count += torch.sum(predictions_argmax == torch.IntTensor(labels).to(device))\n",
        "    total_count += len(labels)\n",
        "  return float(correct_count) / total_count\n",
        "\n",
        "gat = GAT(node_feature_vector_len=example_node.get_feature_vector(node_feature_a_size=node_feature_a_size).shape[0], num_classes=num_classes).to(device)\n",
        "print(f\"Param count {sum(param.numel() for param in gat.parameters())}\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(gat.parameters(), lr=.001)\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Beginning training for epoch {epoch+1} of {num_epochs}...\")\n",
        "  train_set_copy = list(train_set)\n",
        "  random.shuffle(train_set_copy)\n",
        "  for graph, labels in train_set_copy:\n",
        "    A = torch.Tensor(graph.get_adjacency_matrix(add_self_loops=False)).to(device)\n",
        "    all_node_feature_vectors = [\n",
        "        graph.id_to_node[i].get_feature_vector(node_feature_a_size=node_feature_a_size)\n",
        "        for i in range(len(graph.id_to_node))]\n",
        "    h0 = torch.Tensor(np.vstack(all_node_feature_vectors)).to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = gat.forward(h0, A)  # (batch_size, num_classes)\n",
        "    loss = criterion(outputs, torch.LongTensor(labels).to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  accuracy = calculate_accuracy_over_test_set_pytorch(gat, test_set)\n",
        "  print(f\"After {epoch+1} epochs, test accuracy was {accuracy:.2f}. Final batch loss was {loss:.4f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch Geometric"
      ],
      "metadata": {
        "id": "GfAWLR1Qy6lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n",
        "! pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KLIqtaN66T5",
        "outputId": "4e2793c1-df48-4a44-8157-14036a10bc39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.7.14)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.14.1)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data as PygData\n",
        "from torch_geometric.loader import DataLoader as PygDataLoader\n",
        "from torch_geometric.nn import GCNConv, GATConv"
      ],
      "metadata": {
        "id": "40UATOury8SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def convert_graph_to_pyg_data(graph: Graph, labels: list[int]) -> PygData:\n",
        "  # data.x: Node feature matrix with shape [num_nodes, num_node_features]\n",
        "  # data.edge_index: Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
        "  # data.y: Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
        "  all_node_feature_vectors = [\n",
        "      graph.id_to_node[i].get_feature_vector(node_feature_a_size=node_feature_a_size)\n",
        "      for i in range(len(graph.id_to_node))]\n",
        "  h0 = torch.Tensor(np.vstack(all_node_feature_vectors)).to(device)\n",
        "  data = PygData(x=h0, edge_index=graph.get_edge_connections_coo().to(device), y=torch.tensor(labels, dtype=torch.int64).to(device))\n",
        "  data.validate(raise_on_error=True)\n",
        "  return data\n",
        "\n",
        "pyg_dataset = []\n",
        "for graph, labels in train_set:\n",
        "  pyg_dataset.append(convert_graph_to_pyg_data(graph, labels))\n",
        "\n",
        "print(f\"{len(pyg_dataset)} training examples converted.\")\n",
        "print(f\"First graph num nodes: {pyg_dataset[0].num_nodes}\")\n",
        "print(f\"First graph num edges: {pyg_dataset[0].num_edges}\")\n",
        "print(f\"First graph num node features: {pyg_dataset[0].num_node_features}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeqm4moFy_uC",
        "outputId": "3e41a05d-4b8a-42ee-dc83-20000ddcd3de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 training examples converted.\n",
            "First graph num nodes: 7\n",
            "First graph num edges: 36\n",
            "First graph num node features: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds(2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class PygGCN(nn.Module):\n",
        "\n",
        "  def __init__(self, *, node_feature_vector_len: int, num_classes: int):\n",
        "    super().__init__()\n",
        "    # self.dropout_rate = .01  # No dropout worked better than any dropout so disabling here.\n",
        "    self.gcn_conv_1 = GCNConv(node_feature_vector_len, node_feature_vector_len, add_self_loops=False, normalize=False, bias=True)\n",
        "    # This is weird. For some reason adding this self-update like I did manually above\n",
        "    # works significantly (as in, over 30 points better) than just using add_self_loops=True.\n",
        "    # That is likely because information from the node itself has a significant impact on\n",
        "    # its classification (but I feel that would likely often be the case for node classification\n",
        "    # tasks).\n",
        "    self.self_update_1 = nn.Linear(node_feature_vector_len, node_feature_vector_len)\n",
        "    self.gcn_conv_2 = GCNConv(node_feature_vector_len, node_feature_vector_len, add_self_loops=False, normalize=False, bias=True)\n",
        "    self.self_update_2 = nn.Linear(node_feature_vector_len, node_feature_vector_len)\n",
        "    self.classification_head = nn.Sequential(\n",
        "        nn.Linear(node_feature_vector_len, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, num_classes),\n",
        "    )\n",
        "\n",
        "  def forward(self, data: PygData) -> torch.Tensor:\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "    x = self.gcn_conv_1(x, edge_index) + self.self_update_1(x)\n",
        "    x = nn.functional.relu(x)\n",
        "    # x = nn.functional.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "    x = self.gcn_conv_2(x, edge_index) + self.self_update_2(x)\n",
        "    x = nn.functional.relu(x)\n",
        "    # x = nn.functional.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "    return self.classification_head(x)\n",
        "\n",
        "def calculate_accuracy_over_test_set_pytorch_geometric(\n",
        "    params: PygGCN, test_set: list[tuple[Graph, list[int]]]\n",
        ") -> float:\n",
        "  # Returns the percentage of test examples classified correctly.\n",
        "  original_training = params.training\n",
        "  params.eval()\n",
        "  correct_count = 0\n",
        "  total_count = 0\n",
        "  for graph, labels in test_set:\n",
        "    pyg_data = convert_graph_to_pyg_data(graph, labels)\n",
        "    prediction_logits = params.forward(pyg_data)\n",
        "    softmaxed = nn.functional.softmax(prediction_logits, dim=1)\n",
        "    predictions_argmax = torch.argmax(softmaxed, dim=1)\n",
        "    correct_count += torch.sum(predictions_argmax == torch.IntTensor(labels).to(device))\n",
        "    total_count += len(labels)\n",
        "  params.train()\n",
        "  return float(correct_count) / total_count\n",
        "\n",
        "\n",
        "pyg_gcn = PygGCN(node_feature_vector_len=example_node.get_feature_vector(node_feature_a_size=node_feature_a_size).shape[0], num_classes=num_classes).to(device)\n",
        "print(f\"Param count {sum(param.numel() for param in pyg_gcn.parameters())}\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(pyg_gcn.parameters(), lr=.001)\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Beginning training for epoch {epoch+1} of {num_epochs}...\")\n",
        "  train_set_copy = list(pyg_dataset)\n",
        "  random.shuffle(train_set_copy)\n",
        "  # Don't need a Dataset/InMemoryDataset object here since we already have everything\n",
        "  # in memory and don't need to download the dataset or import it from files.\n",
        "  # pytorch_geometric batching works by taking batch_size individual graphs and combining them\n",
        "  # into a large, disconnected graph.\n",
        "  loader = PygDataLoader(train_set_copy, batch_size=32)\n",
        "  for batch in loader:\n",
        "    # https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Batch.html#torch_geometric.data.Batch\n",
        "    # \"torch_geometric.data.Batch inherits from torch_geometric.data.Data and contains an additional attribute called batch\"\n",
        "    optimizer.zero_grad()\n",
        "    outputs = pyg_gcn.forward(batch.to(device))  # (batch_size, num_classes)\n",
        "    loss = criterion(outputs, batch.y.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  accuracy = calculate_accuracy_over_test_set_pytorch_geometric(pyg_gcn, test_set)\n",
        "  print(f\"After {epoch+1} epochs, test accuracy was {accuracy:.2f}. Final batch loss was {loss:.4f}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZT-A8_aoTZr",
        "outputId": "ae05e63b-9972-41d1-9d01-68dfc8c5615d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Param count 3320\n",
            "Beginning training for epoch 1 of 50...\n",
            "After 1 epochs, test accuracy was 0.53. Final batch loss was 1.2818.\n",
            "Beginning training for epoch 2 of 50...\n",
            "After 2 epochs, test accuracy was 0.67. Final batch loss was 0.8820.\n",
            "Beginning training for epoch 3 of 50...\n",
            "After 3 epochs, test accuracy was 0.73. Final batch loss was 0.6417.\n",
            "Beginning training for epoch 4 of 50...\n",
            "After 4 epochs, test accuracy was 0.84. Final batch loss was 0.4031.\n",
            "Beginning training for epoch 5 of 50...\n",
            "After 5 epochs, test accuracy was 0.87. Final batch loss was 0.2279.\n",
            "Beginning training for epoch 6 of 50...\n",
            "After 6 epochs, test accuracy was 0.88. Final batch loss was 0.3545.\n",
            "Beginning training for epoch 7 of 50...\n",
            "After 7 epochs, test accuracy was 0.88. Final batch loss was 0.2972.\n",
            "Beginning training for epoch 8 of 50...\n",
            "After 8 epochs, test accuracy was 0.89. Final batch loss was 0.4528.\n",
            "Beginning training for epoch 9 of 50...\n",
            "After 9 epochs, test accuracy was 0.89. Final batch loss was 0.1584.\n",
            "Beginning training for epoch 10 of 50...\n",
            "After 10 epochs, test accuracy was 0.89. Final batch loss was 0.1892.\n",
            "Beginning training for epoch 11 of 50...\n",
            "After 11 epochs, test accuracy was 0.90. Final batch loss was 0.2095.\n",
            "Beginning training for epoch 12 of 50...\n",
            "After 12 epochs, test accuracy was 0.89. Final batch loss was 0.2905.\n",
            "Beginning training for epoch 13 of 50...\n",
            "After 13 epochs, test accuracy was 0.91. Final batch loss was 0.2342.\n",
            "Beginning training for epoch 14 of 50...\n",
            "After 14 epochs, test accuracy was 0.90. Final batch loss was 0.1219.\n",
            "Beginning training for epoch 15 of 50...\n",
            "After 15 epochs, test accuracy was 0.91. Final batch loss was 0.3764.\n",
            "Beginning training for epoch 16 of 50...\n",
            "After 16 epochs, test accuracy was 0.91. Final batch loss was 0.3233.\n",
            "Beginning training for epoch 17 of 50...\n",
            "After 17 epochs, test accuracy was 0.91. Final batch loss was 0.2978.\n",
            "Beginning training for epoch 18 of 50...\n",
            "After 18 epochs, test accuracy was 0.91. Final batch loss was 0.2790.\n",
            "Beginning training for epoch 19 of 50...\n",
            "After 19 epochs, test accuracy was 0.91. Final batch loss was 0.1284.\n",
            "Beginning training for epoch 20 of 50...\n",
            "After 20 epochs, test accuracy was 0.90. Final batch loss was 0.1590.\n",
            "Beginning training for epoch 21 of 50...\n",
            "After 21 epochs, test accuracy was 0.90. Final batch loss was 0.3500.\n",
            "Beginning training for epoch 22 of 50...\n",
            "After 22 epochs, test accuracy was 0.91. Final batch loss was 0.1311.\n",
            "Beginning training for epoch 23 of 50...\n",
            "After 23 epochs, test accuracy was 0.91. Final batch loss was 0.1465.\n",
            "Beginning training for epoch 24 of 50...\n",
            "After 24 epochs, test accuracy was 0.91. Final batch loss was 0.1513.\n",
            "Beginning training for epoch 25 of 50...\n",
            "After 25 epochs, test accuracy was 0.90. Final batch loss was 0.1729.\n",
            "Beginning training for epoch 26 of 50...\n",
            "After 26 epochs, test accuracy was 0.91. Final batch loss was 0.2636.\n",
            "Beginning training for epoch 27 of 50...\n",
            "After 27 epochs, test accuracy was 0.91. Final batch loss was 0.0830.\n",
            "Beginning training for epoch 28 of 50...\n",
            "After 28 epochs, test accuracy was 0.92. Final batch loss was 0.1266.\n",
            "Beginning training for epoch 29 of 50...\n",
            "After 29 epochs, test accuracy was 0.92. Final batch loss was 0.1969.\n",
            "Beginning training for epoch 30 of 50...\n",
            "After 30 epochs, test accuracy was 0.92. Final batch loss was 0.1948.\n",
            "Beginning training for epoch 31 of 50...\n",
            "After 31 epochs, test accuracy was 0.91. Final batch loss was 0.2407.\n",
            "Beginning training for epoch 32 of 50...\n",
            "After 32 epochs, test accuracy was 0.91. Final batch loss was 0.1991.\n",
            "Beginning training for epoch 33 of 50...\n",
            "After 33 epochs, test accuracy was 0.92. Final batch loss was 0.1315.\n",
            "Beginning training for epoch 34 of 50...\n",
            "After 34 epochs, test accuracy was 0.92. Final batch loss was 0.2455.\n",
            "Beginning training for epoch 35 of 50...\n",
            "After 35 epochs, test accuracy was 0.91. Final batch loss was 0.1864.\n",
            "Beginning training for epoch 36 of 50...\n",
            "After 36 epochs, test accuracy was 0.92. Final batch loss was 0.4262.\n",
            "Beginning training for epoch 37 of 50...\n",
            "After 37 epochs, test accuracy was 0.92. Final batch loss was 0.1233.\n",
            "Beginning training for epoch 38 of 50...\n",
            "After 38 epochs, test accuracy was 0.92. Final batch loss was 0.2428.\n",
            "Beginning training for epoch 39 of 50...\n",
            "After 39 epochs, test accuracy was 0.92. Final batch loss was 0.2450.\n",
            "Beginning training for epoch 40 of 50...\n",
            "After 40 epochs, test accuracy was 0.92. Final batch loss was 0.1341.\n",
            "Beginning training for epoch 41 of 50...\n",
            "After 41 epochs, test accuracy was 0.92. Final batch loss was 0.2685.\n",
            "Beginning training for epoch 42 of 50...\n",
            "After 42 epochs, test accuracy was 0.93. Final batch loss was 0.1901.\n",
            "Beginning training for epoch 43 of 50...\n",
            "After 43 epochs, test accuracy was 0.93. Final batch loss was 0.1885.\n",
            "Beginning training for epoch 44 of 50...\n",
            "After 44 epochs, test accuracy was 0.93. Final batch loss was 0.1678.\n",
            "Beginning training for epoch 45 of 50...\n",
            "After 45 epochs, test accuracy was 0.93. Final batch loss was 0.2487.\n",
            "Beginning training for epoch 46 of 50...\n",
            "After 46 epochs, test accuracy was 0.93. Final batch loss was 0.0983.\n",
            "Beginning training for epoch 47 of 50...\n",
            "After 47 epochs, test accuracy was 0.93. Final batch loss was 0.0401.\n",
            "Beginning training for epoch 48 of 50...\n",
            "After 48 epochs, test accuracy was 0.93. Final batch loss was 0.0960.\n",
            "Beginning training for epoch 49 of 50...\n",
            "After 49 epochs, test accuracy was 0.94. Final batch loss was 0.0719.\n",
            "Beginning training for epoch 50 of 50...\n",
            "After 50 epochs, test accuracy was 0.94. Final batch loss was 0.1323.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds(2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class PygGAT(nn.Module):\n",
        "\n",
        "  def __init__(self, *, node_feature_vector_len: int, num_classes: int):\n",
        "    super().__init__()\n",
        "    # self.dropout_rate = .01  # No dropout worked better than any dropout so disabling here.\n",
        "    num_heads = 2\n",
        "    self.gat_conv_1 = GATConv(node_feature_vector_len, node_feature_vector_len // num_heads, heads=num_heads, concat=True, add_self_loops=False, bias=True)\n",
        "    self.self_update_1 = nn.Linear(node_feature_vector_len, node_feature_vector_len)\n",
        "    self.gat_conv_2 = GATConv(node_feature_vector_len, node_feature_vector_len // num_heads, heads=num_heads, concat=True, add_self_loops=False, bias=True)\n",
        "    self.self_update_2 = nn.Linear(node_feature_vector_len, node_feature_vector_len)\n",
        "    self.classification_head = nn.Sequential(\n",
        "        nn.Linear(node_feature_vector_len, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, num_classes),\n",
        "    )\n",
        "\n",
        "  def forward(self, data: PygData) -> torch.Tensor:\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "    x = self.gat_conv_1(x, edge_index) + self.self_update_1(x)\n",
        "    x = nn.functional.relu(x)\n",
        "    # x = nn.functional.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "    x = self.gat_conv_2(x, edge_index) + self.self_update_2(x)\n",
        "    x = nn.functional.relu(x)\n",
        "    # x = nn.functional.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "    return self.classification_head(x)\n",
        "\n",
        "def calculate_accuracy_over_test_set_pytorch_geometric(\n",
        "    params: PygGAT, test_set: list[tuple[Graph, list[int]]]\n",
        ") -> float:\n",
        "  # Returns the percentage of test examples classified correctly.\n",
        "  original_training = params.training\n",
        "  params.eval()\n",
        "  correct_count = 0\n",
        "  total_count = 0\n",
        "  for graph, labels in test_set:\n",
        "    pyg_data = convert_graph_to_pyg_data(graph, labels)\n",
        "    prediction_logits = params.forward(pyg_data)\n",
        "    softmaxed = nn.functional.softmax(prediction_logits, dim=1)\n",
        "    predictions_argmax = torch.argmax(softmaxed, dim=1)\n",
        "    correct_count += torch.sum(predictions_argmax == torch.IntTensor(labels).to(device))\n",
        "    total_count += len(labels)\n",
        "  params.train()\n",
        "  return float(correct_count) / total_count\n",
        "\n",
        "\n",
        "pyg_gat = PygGAT(node_feature_vector_len=example_node.get_feature_vector(node_feature_a_size=node_feature_a_size).shape[0], num_classes=num_classes).to(device)\n",
        "print(f\"Param count {sum(param.numel() for param in pyg_gat.parameters())}\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(pyg_gat.parameters(), lr=.001)\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Beginning training for epoch {epoch+1} of {num_epochs}...\")\n",
        "  train_set_copy = list(pyg_dataset)\n",
        "  random.shuffle(train_set_copy)\n",
        "  # Don't need a Dataset/InMemoryDataset object here since we already have everything\n",
        "  # in memory and don't need to download the dataset or import it from files.\n",
        "  # pytorch_geometric batching works by taking batch_size individual graphs and combining them\n",
        "  # into a large, disconnected graph.\n",
        "  loader = PygDataLoader(train_set_copy, batch_size=32)\n",
        "  for batch in loader:\n",
        "    # https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Batch.html#torch_geometric.data.Batch\n",
        "    # \"torch_geometric.data.Batch inherits from torch_geometric.data.Data and contains an additional attribute called batch\"\n",
        "    optimizer.zero_grad()\n",
        "    outputs = pyg_gat.forward(batch.to(device))  # (batch_size, num_classes)\n",
        "    loss = criterion(outputs, batch.y.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  accuracy = calculate_accuracy_over_test_set_pytorch_geometric(pyg_gat, test_set)\n",
        "  print(f\"After {epoch+1} epochs, test accuracy was {accuracy:.2f}. Final batch loss was {loss:.4f}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J14GW_7xOFfY",
        "outputId": "5b1f40d8-106e-4f35-9e0f-11024a6100bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Param count 3368\n",
            "Beginning training for epoch 1 of 50...\n",
            "After 1 epochs, test accuracy was 0.57. Final batch loss was 1.1272.\n",
            "Beginning training for epoch 2 of 50...\n",
            "After 2 epochs, test accuracy was 0.64. Final batch loss was 0.9342.\n",
            "Beginning training for epoch 3 of 50...\n",
            "After 3 epochs, test accuracy was 0.66. Final batch loss was 0.8695.\n",
            "Beginning training for epoch 4 of 50...\n",
            "After 4 epochs, test accuracy was 0.68. Final batch loss was 0.6681.\n",
            "Beginning training for epoch 5 of 50...\n",
            "After 5 epochs, test accuracy was 0.69. Final batch loss was 0.5843.\n",
            "Beginning training for epoch 6 of 50...\n",
            "After 6 epochs, test accuracy was 0.71. Final batch loss was 0.7093.\n",
            "Beginning training for epoch 7 of 50...\n",
            "After 7 epochs, test accuracy was 0.72. Final batch loss was 0.7248.\n",
            "Beginning training for epoch 8 of 50...\n",
            "After 8 epochs, test accuracy was 0.74. Final batch loss was 0.7649.\n",
            "Beginning training for epoch 9 of 50...\n",
            "After 9 epochs, test accuracy was 0.74. Final batch loss was 0.5400.\n",
            "Beginning training for epoch 10 of 50...\n",
            "After 10 epochs, test accuracy was 0.74. Final batch loss was 0.5971.\n",
            "Beginning training for epoch 11 of 50...\n",
            "After 11 epochs, test accuracy was 0.75. Final batch loss was 0.5550.\n",
            "Beginning training for epoch 12 of 50...\n",
            "After 12 epochs, test accuracy was 0.76. Final batch loss was 0.7836.\n",
            "Beginning training for epoch 13 of 50...\n",
            "After 13 epochs, test accuracy was 0.77. Final batch loss was 0.5699.\n",
            "Beginning training for epoch 14 of 50...\n",
            "After 14 epochs, test accuracy was 0.77. Final batch loss was 0.4336.\n",
            "Beginning training for epoch 15 of 50...\n",
            "After 15 epochs, test accuracy was 0.76. Final batch loss was 0.5625.\n",
            "Beginning training for epoch 16 of 50...\n",
            "After 16 epochs, test accuracy was 0.78. Final batch loss was 0.4731.\n",
            "Beginning training for epoch 17 of 50...\n",
            "After 17 epochs, test accuracy was 0.78. Final batch loss was 0.5025.\n",
            "Beginning training for epoch 18 of 50...\n",
            "After 18 epochs, test accuracy was 0.79. Final batch loss was 0.4979.\n",
            "Beginning training for epoch 19 of 50...\n",
            "After 19 epochs, test accuracy was 0.81. Final batch loss was 0.5055.\n",
            "Beginning training for epoch 20 of 50...\n",
            "After 20 epochs, test accuracy was 0.80. Final batch loss was 0.3426.\n",
            "Beginning training for epoch 21 of 50...\n",
            "After 21 epochs, test accuracy was 0.81. Final batch loss was 0.4621.\n",
            "Beginning training for epoch 22 of 50...\n",
            "After 22 epochs, test accuracy was 0.82. Final batch loss was 0.3762.\n",
            "Beginning training for epoch 23 of 50...\n",
            "After 23 epochs, test accuracy was 0.82. Final batch loss was 0.3634.\n",
            "Beginning training for epoch 24 of 50...\n",
            "After 24 epochs, test accuracy was 0.82. Final batch loss was 0.3688.\n",
            "Beginning training for epoch 25 of 50...\n",
            "After 25 epochs, test accuracy was 0.81. Final batch loss was 0.4068.\n",
            "Beginning training for epoch 26 of 50...\n",
            "After 26 epochs, test accuracy was 0.83. Final batch loss was 0.5596.\n",
            "Beginning training for epoch 27 of 50...\n",
            "After 27 epochs, test accuracy was 0.83. Final batch loss was 0.3425.\n",
            "Beginning training for epoch 28 of 50...\n",
            "After 28 epochs, test accuracy was 0.83. Final batch loss was 0.2266.\n",
            "Beginning training for epoch 29 of 50...\n",
            "After 29 epochs, test accuracy was 0.83. Final batch loss was 0.3078.\n",
            "Beginning training for epoch 30 of 50...\n",
            "After 30 epochs, test accuracy was 0.83. Final batch loss was 0.3627.\n",
            "Beginning training for epoch 31 of 50...\n",
            "After 31 epochs, test accuracy was 0.83. Final batch loss was 0.5076.\n",
            "Beginning training for epoch 32 of 50...\n",
            "After 32 epochs, test accuracy was 0.85. Final batch loss was 0.3568.\n",
            "Beginning training for epoch 33 of 50...\n",
            "After 33 epochs, test accuracy was 0.84. Final batch loss was 0.2929.\n",
            "Beginning training for epoch 34 of 50...\n",
            "After 34 epochs, test accuracy was 0.85. Final batch loss was 0.4931.\n",
            "Beginning training for epoch 35 of 50...\n",
            "After 35 epochs, test accuracy was 0.84. Final batch loss was 0.3034.\n",
            "Beginning training for epoch 36 of 50...\n",
            "After 36 epochs, test accuracy was 0.85. Final batch loss was 0.3872.\n",
            "Beginning training for epoch 37 of 50...\n",
            "After 37 epochs, test accuracy was 0.85. Final batch loss was 0.4104.\n",
            "Beginning training for epoch 38 of 50...\n",
            "After 38 epochs, test accuracy was 0.85. Final batch loss was 0.3332.\n",
            "Beginning training for epoch 39 of 50...\n",
            "After 39 epochs, test accuracy was 0.85. Final batch loss was 0.3039.\n",
            "Beginning training for epoch 40 of 50...\n",
            "After 40 epochs, test accuracy was 0.86. Final batch loss was 0.2288.\n",
            "Beginning training for epoch 41 of 50...\n",
            "After 41 epochs, test accuracy was 0.85. Final batch loss was 0.3664.\n",
            "Beginning training for epoch 42 of 50...\n",
            "After 42 epochs, test accuracy was 0.86. Final batch loss was 0.4150.\n",
            "Beginning training for epoch 43 of 50...\n",
            "After 43 epochs, test accuracy was 0.86. Final batch loss was 0.2489.\n",
            "Beginning training for epoch 44 of 50...\n",
            "After 44 epochs, test accuracy was 0.86. Final batch loss was 0.2500.\n",
            "Beginning training for epoch 45 of 50...\n",
            "After 45 epochs, test accuracy was 0.87. Final batch loss was 0.3658.\n",
            "Beginning training for epoch 46 of 50...\n",
            "After 46 epochs, test accuracy was 0.85. Final batch loss was 0.2855.\n",
            "Beginning training for epoch 47 of 50...\n",
            "After 47 epochs, test accuracy was 0.87. Final batch loss was 0.2535.\n",
            "Beginning training for epoch 48 of 50...\n",
            "After 48 epochs, test accuracy was 0.86. Final batch loss was 0.2965.\n",
            "Beginning training for epoch 49 of 50...\n",
            "After 49 epochs, test accuracy was 0.87. Final batch loss was 0.2804.\n",
            "Beginning training for epoch 50 of 50...\n",
            "After 50 epochs, test accuracy was 0.86. Final batch loss was 0.3261.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPvF8FCUFzNsfpBDHWmgvJY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}